{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","gpuClass":"standard","widgets":{"application/vnd.jupyter.widget-state+json":{"b373a18f432541eeaa4b94879a3a8241":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_62929bf449c74270a916b3e0243c8241","IPY_MODEL_fb972eae15104b728d3ec3aed8cb5774","IPY_MODEL_873b15f41f5745f8bc6a10589b88044f"],"layout":"IPY_MODEL_45fd6e32afb04857b2227990a478d37f"}},"62929bf449c74270a916b3e0243c8241":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_c2b113b5b9bf455693fa8b095e6eac7e","placeholder":"​","style":"IPY_MODEL_de32588a0f724112aa2e09920deca398","value":"Downloading (…)rocessor_config.json: 100%"}},"fb972eae15104b728d3ec3aed8cb5774":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_65676abcb9bb4e239386921c37ee72a4","max":433,"min":0,"orientation":"horizontal","style":"IPY_MODEL_bbcec16c89b24f09bdfd918b145ecadd","value":433}},"873b15f41f5745f8bc6a10589b88044f":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_a84bae4accbe451c89681141362e25e7","placeholder":"​","style":"IPY_MODEL_943c016e142d42a09ecd2e96d73a4d54","value":" 433/433 [00:00&lt;00:00, 28.7kB/s]"}},"45fd6e32afb04857b2227990a478d37f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c2b113b5b9bf455693fa8b095e6eac7e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"de32588a0f724112aa2e09920deca398":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"65676abcb9bb4e239386921c37ee72a4":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"bbcec16c89b24f09bdfd918b145ecadd":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"a84bae4accbe451c89681141362e25e7":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"943c016e142d42a09ecd2e96d73a4d54":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"88d92b6abc5b4e52b04d81ef2e3fad57":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_d1f6924fab05495d8a6e574012e74d1f","IPY_MODEL_c707815002b944aca410092614745b6f","IPY_MODEL_077033813d904f3abeae9a704c37fd77"],"layout":"IPY_MODEL_54949c4f1d9544b8adf67c405e1ea83c"}},"d1f6924fab05495d8a6e574012e74d1f":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_109e5aaf06f84cb195e269291d872c83","placeholder":"​","style":"IPY_MODEL_1cbe285c8af747d29a9d66590229b0ca","value":"Downloading spm_char.model: 100%"}},"c707815002b944aca410092614745b6f":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_6936f4fe05234828817f91678790ff0e","max":238473,"min":0,"orientation":"horizontal","style":"IPY_MODEL_0f50c505eb894c718bef0f4598507cd7","value":238473}},"077033813d904f3abeae9a704c37fd77":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_027e0816746e4507b868081f21979d98","placeholder":"​","style":"IPY_MODEL_566b01ddf8ee460b9262949a443fd814","value":" 238k/238k [00:00&lt;00:00, 462kB/s]"}},"54949c4f1d9544b8adf67c405e1ea83c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"109e5aaf06f84cb195e269291d872c83":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1cbe285c8af747d29a9d66590229b0ca":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"6936f4fe05234828817f91678790ff0e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0f50c505eb894c718bef0f4598507cd7":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"027e0816746e4507b868081f21979d98":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"566b01ddf8ee460b9262949a443fd814":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"0ee95be406ff4b55bf8815df9acfab7b":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_32a9607aa01843e097eb161a53cfd708","IPY_MODEL_92d2b819f9b04c389f11ac9d53b54250","IPY_MODEL_5da911c12f004b1fb0fb84bb14caae2b"],"layout":"IPY_MODEL_13df5f534a4f414c8cbe3e58cb6850a5"}},"32a9607aa01843e097eb161a53cfd708":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_ab11520441c24c32b8f47e66bb834d2b","placeholder":"​","style":"IPY_MODEL_885078e7cb684550b1e530f6df85897b","value":"Downloading (…)in/added_tokens.json: 100%"}},"92d2b819f9b04c389f11ac9d53b54250":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_51db7180036546cd8f5b59f27d72a944","max":40,"min":0,"orientation":"horizontal","style":"IPY_MODEL_9dbf17817d5846ddbb1b74b5950a022e","value":40}},"5da911c12f004b1fb0fb84bb14caae2b":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_5638c69d52d64da784f090b3fecb69c3","placeholder":"​","style":"IPY_MODEL_bda4745e2be44d15b3fb9848dabea833","value":" 40.0/40.0 [00:00&lt;00:00, 2.97kB/s]"}},"13df5f534a4f414c8cbe3e58cb6850a5":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ab11520441c24c32b8f47e66bb834d2b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"885078e7cb684550b1e530f6df85897b":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"51db7180036546cd8f5b59f27d72a944":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9dbf17817d5846ddbb1b74b5950a022e":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"5638c69d52d64da784f090b3fecb69c3":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"bda4745e2be44d15b3fb9848dabea833":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"8c00e3f928724cfdbc2229feec283894":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_982c880b765d452ba5ebf6949d4cf782","IPY_MODEL_971ee60309bf4fd88832a93d0f0b7588","IPY_MODEL_fa1ed0d3bfec41e79cf21ed70751d5e4"],"layout":"IPY_MODEL_a4d00cf619e04350b885864ac0f74956"}},"982c880b765d452ba5ebf6949d4cf782":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_674e2c0143e04e45a99402b91dd5ccbf","placeholder":"​","style":"IPY_MODEL_527a02c531e342d5b5c581b8ceb368d1","value":"Downloading (…)cial_tokens_map.json: 100%"}},"971ee60309bf4fd88832a93d0f0b7588":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_b4986b132106477c930a80bf9994482e","max":234,"min":0,"orientation":"horizontal","style":"IPY_MODEL_97373a161f70467499cb572f199327aa","value":234}},"fa1ed0d3bfec41e79cf21ed70751d5e4":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_fa09cc15bd934173abd11dda35d0adc8","placeholder":"​","style":"IPY_MODEL_f38819aeb9c24017886ef900cbbcc499","value":" 234/234 [00:00&lt;00:00, 11.9kB/s]"}},"a4d00cf619e04350b885864ac0f74956":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"674e2c0143e04e45a99402b91dd5ccbf":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"527a02c531e342d5b5c581b8ceb368d1":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"b4986b132106477c930a80bf9994482e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"97373a161f70467499cb572f199327aa":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"fa09cc15bd934173abd11dda35d0adc8":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f38819aeb9c24017886ef900cbbcc499":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"715b0ca6dcea4800af4d71fb45b812d2":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_c3f5b16bb4cd451ba632760bdaae847e","IPY_MODEL_757a1f6210374ff1a6ad15fef45807a1","IPY_MODEL_6fb7d8b3eb1b43f9ac4dc65d0fe763b8"],"layout":"IPY_MODEL_48099cf01e8e4ccebd298e526cd8c159"}},"c3f5b16bb4cd451ba632760bdaae847e":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_f5b7074978074a6c8a5400a1e373975b","placeholder":"​","style":"IPY_MODEL_1702de65d6584a558e885e805ced5b24","value":"Downloading (…)okenizer_config.json: 100%"}},"757a1f6210374ff1a6ad15fef45807a1":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_2d18dc538ee44f81ba32593bc39b8ad2","max":232,"min":0,"orientation":"horizontal","style":"IPY_MODEL_bc030b008696413c97821a26620627c6","value":232}},"6fb7d8b3eb1b43f9ac4dc65d0fe763b8":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_fb168617ce324fb1a7f49e9d7431837a","placeholder":"​","style":"IPY_MODEL_097cdf22fbdf4625960cb2de224cfb0c","value":" 232/232 [00:00&lt;00:00, 6.70kB/s]"}},"48099cf01e8e4ccebd298e526cd8c159":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f5b7074978074a6c8a5400a1e373975b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1702de65d6584a558e885e805ced5b24":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"2d18dc538ee44f81ba32593bc39b8ad2":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"bc030b008696413c97821a26620627c6":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"fb168617ce324fb1a7f49e9d7431837a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"097cdf22fbdf4625960cb2de224cfb0c":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"c75d3ac02ebb4a7fb79a28d737b09eec":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_ac293583355f4b1da9e11dc6c8c1a567","IPY_MODEL_ccf3da853c114c68b7c8e6a95c8e38d4","IPY_MODEL_f91c845cf3ca46029021f6b928497718"],"layout":"IPY_MODEL_7ad76d011036402faebc8948509d3c3b"}},"ac293583355f4b1da9e11dc6c8c1a567":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_2ad3404544244e7e965c1137ed38ae6d","placeholder":"​","style":"IPY_MODEL_4fdcbe20fa604b399bcc2758141c168f","value":"Downloading (…)lve/main/config.json: 100%"}},"ccf3da853c114c68b7c8e6a95c8e38d4":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_8e7f888f480f4cb1a47f9b42af83a31a","max":2062,"min":0,"orientation":"horizontal","style":"IPY_MODEL_83dd9d35b89044cb97c3f8f79826ee69","value":2062}},"f91c845cf3ca46029021f6b928497718":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_d2c009baa5cc4a199a41338482aaa8d9","placeholder":"​","style":"IPY_MODEL_40c2508b39f54fd5881cc863c13e7ad0","value":" 2.06k/2.06k [00:00&lt;00:00, 101kB/s]"}},"7ad76d011036402faebc8948509d3c3b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2ad3404544244e7e965c1137ed38ae6d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4fdcbe20fa604b399bcc2758141c168f":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"8e7f888f480f4cb1a47f9b42af83a31a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"83dd9d35b89044cb97c3f8f79826ee69":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"d2c009baa5cc4a199a41338482aaa8d9":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"40c2508b39f54fd5881cc863c13e7ad0":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"9c55bafb27d4411fab0155638ac69194":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_b1ef4cf6b3dd46cfaf66b255dccf01b2","IPY_MODEL_b8aa27b690b0414f8506129a943b41ab","IPY_MODEL_2498039a46e0485c8d5815b37c25694b"],"layout":"IPY_MODEL_5010a0ecd9594bcebf563c0a23bdfb09"}},"b1ef4cf6b3dd46cfaf66b255dccf01b2":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_21f0ec50791a401a9384cebb95e945cb","placeholder":"​","style":"IPY_MODEL_c82e6547051044e0ab8df397d2ca7d42","value":"Downloading pytorch_model.bin: 100%"}},"b8aa27b690b0414f8506129a943b41ab":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_7ab8f31a9e5146a594223f06a7e4343d","max":585476837,"min":0,"orientation":"horizontal","style":"IPY_MODEL_32511e7a13f344c595e1aac9684c3391","value":585476837}},"2498039a46e0485c8d5815b37c25694b":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_7afe905fb88c433ba811c8f6455a5576","placeholder":"​","style":"IPY_MODEL_68ac5d1b71ca4b7aac2ed33d4e97a062","value":" 585M/585M [00:26&lt;00:00, 23.6MB/s]"}},"5010a0ecd9594bcebf563c0a23bdfb09":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"21f0ec50791a401a9384cebb95e945cb":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c82e6547051044e0ab8df397d2ca7d42":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"7ab8f31a9e5146a594223f06a7e4343d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"32511e7a13f344c595e1aac9684c3391":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"7afe905fb88c433ba811c8f6455a5576":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"68ac5d1b71ca4b7aac2ed33d4e97a062":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"2952a39ee3f04d378b492c2d312b64d2":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_08186d80349b463cbe067838882c3022","IPY_MODEL_beda7be92eec4a4693caeeafb14162d5","IPY_MODEL_8daa3dd103d54ae48dcd6db67f9fa7a3"],"layout":"IPY_MODEL_469f5749d0cd44a597458b7ab135a556"}},"08186d80349b463cbe067838882c3022":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_666c452bb5cd48ba833d48ef581319bb","placeholder":"​","style":"IPY_MODEL_617a33922ea9446582f89d391b2a5fb4","value":"Resolving data files: 100%"}},"beda7be92eec4a4693caeeafb14162d5":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_cc0354c0f8b44776ac0417bd0f4191b0","max":558,"min":0,"orientation":"horizontal","style":"IPY_MODEL_28246d1f9ccb440ba532d63b350deeb0","value":558}},"8daa3dd103d54ae48dcd6db67f9fa7a3":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_21c376345c1047329b38e69631e1250d","placeholder":"​","style":"IPY_MODEL_b99f824c5b6f4211848569a995dd475a","value":" 558/558 [00:00&lt;00:00, 1591.79it/s]"}},"469f5749d0cd44a597458b7ab135a556":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"666c452bb5cd48ba833d48ef581319bb":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"617a33922ea9446582f89d391b2a5fb4":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"cc0354c0f8b44776ac0417bd0f4191b0":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"28246d1f9ccb440ba532d63b350deeb0":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"21c376345c1047329b38e69631e1250d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b99f824c5b6f4211848569a995dd475a":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"d3a43708fa88406aa8188b33845df847":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_8ca48ed597c84d4f8f2eb4772b4ba6f2","IPY_MODEL_5fe04d4d9fbc49d7a5496cbce4e62912","IPY_MODEL_ee5c5852c0ef468c861d1d7c6ecfc548"],"layout":"IPY_MODEL_1504d8848e074d23899b996a7e3359bc"}},"8ca48ed597c84d4f8f2eb4772b4ba6f2":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_a52a49a08248436082d7546d8ab50a8d","placeholder":"​","style":"IPY_MODEL_9ee7ac5d7b86420e95cf563e8a540fad","value":"Downloading data files: 100%"}},"5fe04d4d9fbc49d7a5496cbce4e62912":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_f9e9ed2525d44563a75408aa0c3f57e3","max":558,"min":0,"orientation":"horizontal","style":"IPY_MODEL_e18b544754e14aada41ae7f8a5e50962","value":558}},"ee5c5852c0ef468c861d1d7c6ecfc548":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_1eef9bbe6dba447aa91c66d0a2418ff8","placeholder":"​","style":"IPY_MODEL_3080ba74016a43f996ea766588fa8f92","value":" 558/558 [00:00&lt;00:00, 2494.64it/s]"}},"1504d8848e074d23899b996a7e3359bc":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a52a49a08248436082d7546d8ab50a8d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9ee7ac5d7b86420e95cf563e8a540fad":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"f9e9ed2525d44563a75408aa0c3f57e3":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e18b544754e14aada41ae7f8a5e50962":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"1eef9bbe6dba447aa91c66d0a2418ff8":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3080ba74016a43f996ea766588fa8f92":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"5add7f86eede40bf9e02a188c9b832f4":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_e7e23af165fb4696838eb088a4ea0d41","IPY_MODEL_09bc2fbad114428e9572a09b3679fdf5","IPY_MODEL_c1ac29e03de64904932c2fa5dab26b8e"],"layout":"IPY_MODEL_ce61f7fa8e8f4bb29374b84aa7beac8c"}},"e7e23af165fb4696838eb088a4ea0d41":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_7821e578a7cc48ec9fa2af276210813d","placeholder":"​","style":"IPY_MODEL_89585dc820c7452283e6cb54be9efa73","value":"Downloading data files: "}},"09bc2fbad114428e9572a09b3679fdf5":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_a87a94f4958d44589870594e5e72476d","max":1,"min":0,"orientation":"horizontal","style":"IPY_MODEL_f62da19f8425497e82d129228f14ccaf","value":0}},"c1ac29e03de64904932c2fa5dab26b8e":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_d7450e3728504ff1bd3fa4277a0a1b81","placeholder":"​","style":"IPY_MODEL_384d1c1be891428f879e147e16a991d3","value":" 0/0 [00:00&lt;?, ?it/s]"}},"ce61f7fa8e8f4bb29374b84aa7beac8c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7821e578a7cc48ec9fa2af276210813d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"89585dc820c7452283e6cb54be9efa73":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"a87a94f4958d44589870594e5e72476d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":"20px"}},"f62da19f8425497e82d129228f14ccaf":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"d7450e3728504ff1bd3fa4277a0a1b81":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"384d1c1be891428f879e147e16a991d3":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"f1cae5f4dacb41298c4ca0a212355fd1":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_a3bf51a9e1284a8f83ef19b09d3c7d0d","IPY_MODEL_3129b8b060564a1c83656c3b263c3a94","IPY_MODEL_913e407dd3844f289ed07448d90cf422"],"layout":"IPY_MODEL_1c226373f546484ca15f603d9095447f"}},"a3bf51a9e1284a8f83ef19b09d3c7d0d":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_84f69df8a8f943ac89201da0cef30ddf","placeholder":"​","style":"IPY_MODEL_4c5096fb48a84eaa9bc4044cd71098a8","value":"Extracting data files: "}},"3129b8b060564a1c83656c3b263c3a94":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_b132e34445c44bb3b426d6be6bd32e3b","max":1,"min":0,"orientation":"horizontal","style":"IPY_MODEL_f9602086721143e8a3f5a615c557d72b","value":0}},"913e407dd3844f289ed07448d90cf422":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_cf10c2b66eec4d8eac67dd44470af183","placeholder":"​","style":"IPY_MODEL_a0eb47bf6ae64b57aa13cb05ac417386","value":" 0/0 [00:00&lt;?, ?it/s]"}},"1c226373f546484ca15f603d9095447f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"84f69df8a8f943ac89201da0cef30ddf":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4c5096fb48a84eaa9bc4044cd71098a8":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"b132e34445c44bb3b426d6be6bd32e3b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":"20px"}},"f9602086721143e8a3f5a615c557d72b":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"cf10c2b66eec4d8eac67dd44470af183":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a0eb47bf6ae64b57aa13cb05ac417386":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"dbbbba84fa424ae38f4c80eaa13a7c2b":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_11d3a4b025f342138c616f2f31df1aad","IPY_MODEL_ea30b1b190e145fa89a94560a915a68c","IPY_MODEL_5bab135460ee4723829de34f5e4cffaa"],"layout":"IPY_MODEL_7fea282189cc426f92cab2b4d7aa683e"}},"11d3a4b025f342138c616f2f31df1aad":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_1fe418d5bfff4fddbb87416fd39b3a9d","placeholder":"​","style":"IPY_MODEL_efd32706490f4172846f491dfcfaea71","value":"Generating train split: "}},"ea30b1b190e145fa89a94560a915a68c":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"info","description":"","description_tooltip":null,"layout":"IPY_MODEL_55e5447c14904c8286417d6f99377d65","max":1,"min":0,"orientation":"horizontal","style":"IPY_MODEL_9f9d1823475f4d73a0b20ef615320fdb","value":1}},"5bab135460ee4723829de34f5e4cffaa":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_9e046391c61142acb44f2486bfc3c83c","placeholder":"​","style":"IPY_MODEL_9e3d78978f3f43bfb680591b7ea7cbe2","value":" 0/0 [00:00&lt;?, ? examples/s]"}},"7fea282189cc426f92cab2b4d7aa683e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":"hidden","width":null}},"1fe418d5bfff4fddbb87416fd39b3a9d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"efd32706490f4172846f491dfcfaea71":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"55e5447c14904c8286417d6f99377d65":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":"20px"}},"9f9d1823475f4d73a0b20ef615320fdb":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"9e046391c61142acb44f2486bfc3c83c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9e3d78978f3f43bfb680591b7ea7cbe2":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"3d0f69de10be4bdb846c692a912bd78d":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_9d679e4e80b94f9c90dafc40faa662ed","IPY_MODEL_89b95ba68c5b4b4f9c5d8b39452a6bcc","IPY_MODEL_42a35630246846d7bb5e76758e01457e"],"layout":"IPY_MODEL_5c4f9929950e4b73b7d06107218eef57"}},"9d679e4e80b94f9c90dafc40faa662ed":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_dd93a5f01c57492789e969b4b1ba7684","placeholder":"​","style":"IPY_MODEL_b55ea99aca5c4390831d621dbbdb94ce","value":"100%"}},"89b95ba68c5b4b4f9c5d8b39452a6bcc":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_6aa0627fceef442395bc52a0e5ecbbf4","max":1,"min":0,"orientation":"horizontal","style":"IPY_MODEL_8d240f5e8daf464f8a2530349d401f13","value":1}},"42a35630246846d7bb5e76758e01457e":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_3912ab8b602e4ce486de8a6bbc575760","placeholder":"​","style":"IPY_MODEL_ada621d9abd84815bfc8f46f4936e7f7","value":" 1/1 [00:00&lt;00:00, 42.03it/s]"}},"5c4f9929950e4b73b7d06107218eef57":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"dd93a5f01c57492789e969b4b1ba7684":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b55ea99aca5c4390831d621dbbdb94ce":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"6aa0627fceef442395bc52a0e5ecbbf4":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8d240f5e8daf464f8a2530349d401f13":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"3912ab8b602e4ce486de8a6bbc575760":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ada621d9abd84815bfc8f46f4936e7f7":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"f59174495a8f4500a485f6f3414018bd":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_9c23888e2cf84195a50f51bb29334c7f","IPY_MODEL_c9f751993e3d4b7ea2e63a6b016da812","IPY_MODEL_0a998db71e264187bd11af34da1db511"],"layout":"IPY_MODEL_e31bb7e85f3043e9bb403e5ac1dcd916"}},"9c23888e2cf84195a50f51bb29334c7f":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_c89edd18b12e49928c3bf57bd89ef4f8","placeholder":"​","style":"IPY_MODEL_a418823a9e4f4d99b7c0ce0f39fd69e8","value":"Map:   0%"}},"c9f751993e3d4b7ea2e63a6b016da812":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"","description":"","description_tooltip":null,"layout":"IPY_MODEL_ac23235623ad4f039c742422dcf027c8","max":557,"min":0,"orientation":"horizontal","style":"IPY_MODEL_8079a9c2f66441ba8fcf0452c2b4d02f","value":557}},"0a998db71e264187bd11af34da1db511":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_d5ec8ab7f8fb47de89e3eba945adde4b","placeholder":"​","style":"IPY_MODEL_98297d9b88dc48abb6df2f6ad561e5e0","value":" 0/557 [00:00&lt;?, ? examples/s]"}},"e31bb7e85f3043e9bb403e5ac1dcd916":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":"hidden","width":null}},"c89edd18b12e49928c3bf57bd89ef4f8":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a418823a9e4f4d99b7c0ce0f39fd69e8":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"ac23235623ad4f039c742422dcf027c8":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8079a9c2f66441ba8fcf0452c2b4d02f":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"d5ec8ab7f8fb47de89e3eba945adde4b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"98297d9b88dc48abb6df2f6ad561e5e0":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"a49a65b432a6481081537d782b58e8f4":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_8debf062cb264e389cc04a8cb812b507","IPY_MODEL_e65aa4c7f19a48c7865c20b88039f3ee","IPY_MODEL_62c9bf462df0452696a79cd57170ea7b"],"layout":"IPY_MODEL_39729571189540069f0308ba54d7b9b1"}},"8debf062cb264e389cc04a8cb812b507":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_7bcb1e55f3eb4f57b07f0c720d6d18d7","placeholder":"​","style":"IPY_MODEL_92f3ab6a7638446f97ae464b6ce4886b","value":"Map:   0%"}},"e65aa4c7f19a48c7865c20b88039f3ee":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"","description":"","description_tooltip":null,"layout":"IPY_MODEL_53f3e27f8a994852b8b54aea52ea5c99","max":557,"min":0,"orientation":"horizontal","style":"IPY_MODEL_c2ae062f965e466cbd5aee10d3f63917","value":557}},"62c9bf462df0452696a79cd57170ea7b":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_99630af0117e442aa2b0841db651a303","placeholder":"​","style":"IPY_MODEL_95f000daa78643729ad8e7aa681afef6","value":" 0/557 [00:00&lt;?, ? examples/s]"}},"39729571189540069f0308ba54d7b9b1":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":"hidden","width":null}},"7bcb1e55f3eb4f57b07f0c720d6d18d7":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"92f3ab6a7638446f97ae464b6ce4886b":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"53f3e27f8a994852b8b54aea52ea5c99":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c2ae062f965e466cbd5aee10d3f63917":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"99630af0117e442aa2b0841db651a303":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"95f000daa78643729ad8e7aa681afef6":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"dc09363fc7054e51986cc8c0e9fbca06":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_510cf9f5620045a6be6de30bb53c85b5","IPY_MODEL_2a065c12a0a74812b08070163b5d456f","IPY_MODEL_c0b8287111f441b7b0bd6266a7877113"],"layout":"IPY_MODEL_2cd0295726704782a836ef7c1b8e2e99"}},"510cf9f5620045a6be6de30bb53c85b5":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_9d33dbc5d23a4bc0a3559e11a7bb7977","placeholder":"​","style":"IPY_MODEL_1a04da82e6fc464dadb4a0183097e2c8","value":"Downloading (…)ain/hyperparams.yaml: 100%"}},"2a065c12a0a74812b08070163b5d456f":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_bf0dcd8bbb794d159921050129f9242f","max":2041,"min":0,"orientation":"horizontal","style":"IPY_MODEL_8dc49415296d410bb39a99941444955c","value":2041}},"c0b8287111f441b7b0bd6266a7877113":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_961fa51ebf464e00b47cb1d4adca5075","placeholder":"​","style":"IPY_MODEL_bd84ff61e0254fe5af6ed21f81a48b14","value":" 2.04k/2.04k [00:00&lt;00:00, 159kB/s]"}},"2cd0295726704782a836ef7c1b8e2e99":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9d33dbc5d23a4bc0a3559e11a7bb7977":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1a04da82e6fc464dadb4a0183097e2c8":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"bf0dcd8bbb794d159921050129f9242f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8dc49415296d410bb39a99941444955c":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"961fa51ebf464e00b47cb1d4adca5075":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"bd84ff61e0254fe5af6ed21f81a48b14":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"33a478781f1441f4a572175dfc99effd":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_e27335bf91a14fcf9c0666e1a099817d","IPY_MODEL_a90a31e3257d4ee4a22645979b9af033","IPY_MODEL_15e80cf16d844328a1eb468fe98841d1"],"layout":"IPY_MODEL_71b9e52c263a4770afcf5d261710cff3"}},"e27335bf91a14fcf9c0666e1a099817d":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_03531ee7155e43a4af67c38e3570d82d","placeholder":"​","style":"IPY_MODEL_1dea68e5a7a942e6955f8bfccc4018fb","value":"Downloading embedding_model.ckpt: 100%"}},"a90a31e3257d4ee4a22645979b9af033":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_ceca5a67ca384a0fb46bbc262d7b12b2","max":16887676,"min":0,"orientation":"horizontal","style":"IPY_MODEL_c11b42c13c144890a0a5ddebc1a81ccd","value":16887676}},"15e80cf16d844328a1eb468fe98841d1":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_345410d0c83b4bc6bf78b48a4cb02bf4","placeholder":"​","style":"IPY_MODEL_0dc3fc0c2fed447dacdcee808c24a257","value":" 16.9M/16.9M [00:01&lt;00:00, 10.8MB/s]"}},"71b9e52c263a4770afcf5d261710cff3":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"03531ee7155e43a4af67c38e3570d82d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1dea68e5a7a942e6955f8bfccc4018fb":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"ceca5a67ca384a0fb46bbc262d7b12b2":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c11b42c13c144890a0a5ddebc1a81ccd":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"345410d0c83b4bc6bf78b48a4cb02bf4":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0dc3fc0c2fed447dacdcee808c24a257":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"ad17c7593f7149d6960546a0af76697e":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_b29a7a23080b47a9a4b1e9eeb1c0d03b","IPY_MODEL_febea99590de446f9e657dfa4e7a1e0c","IPY_MODEL_2e2b091bbcb547b480464df3c7198ca5"],"layout":"IPY_MODEL_1a6bdedc739e4def8f728b49dfd5bedb"}},"b29a7a23080b47a9a4b1e9eeb1c0d03b":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_9062d8f2a5eb4e6d8f6f923c600cb4fa","placeholder":"​","style":"IPY_MODEL_df740b0e83594211937ad0cf8daef22a","value":"Downloading mean_var_norm_emb.ckpt: 100%"}},"febea99590de446f9e657dfa4e7a1e0c":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_1c884795070d43e7b8e7a3e1aec3781d","max":3201,"min":0,"orientation":"horizontal","style":"IPY_MODEL_8486ae6bee324cfcbb66ef122e03804e","value":3201}},"2e2b091bbcb547b480464df3c7198ca5":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_1f0cea87d24541aaabaaafc595a04eec","placeholder":"​","style":"IPY_MODEL_13e292072f5e49a5b6fdd266bb4a42ad","value":" 3.20k/3.20k [00:00&lt;00:00, 66.7kB/s]"}},"1a6bdedc739e4def8f728b49dfd5bedb":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9062d8f2a5eb4e6d8f6f923c600cb4fa":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"df740b0e83594211937ad0cf8daef22a":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"1c884795070d43e7b8e7a3e1aec3781d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8486ae6bee324cfcbb66ef122e03804e":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"1f0cea87d24541aaabaaafc595a04eec":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"13e292072f5e49a5b6fdd266bb4a42ad":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"0ba285903a274165bf45f9ba3e9248f4":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_8fc76868e81b44d3bde0a89daf70b3bf","IPY_MODEL_4b583464f1184a14bdfde52a6d2939a8","IPY_MODEL_846fbd870a7a4b7eaad632333c143643"],"layout":"IPY_MODEL_abda540af1d54f7084fbf4a1e2c102af"}},"8fc76868e81b44d3bde0a89daf70b3bf":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_66eea50b658848a7a2ef6ae3f878b1b3","placeholder":"​","style":"IPY_MODEL_a4848b2e22704e0db39286316a968e47","value":"Downloading classifier.ckpt: 100%"}},"4b583464f1184a14bdfde52a6d2939a8":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_3a926a31134b4f22ae9d9c8bfffa9fd2","max":15856877,"min":0,"orientation":"horizontal","style":"IPY_MODEL_cf7a9f2680db4e73b568b734bc06c96e","value":15856877}},"846fbd870a7a4b7eaad632333c143643":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_59ca467caa3a4f82840a33e5fbc51ce4","placeholder":"​","style":"IPY_MODEL_0b7fea5795f642a89c5f7710532f4372","value":" 15.9M/15.9M [00:01&lt;00:00, 10.1MB/s]"}},"abda540af1d54f7084fbf4a1e2c102af":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"66eea50b658848a7a2ef6ae3f878b1b3":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a4848b2e22704e0db39286316a968e47":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"3a926a31134b4f22ae9d9c8bfffa9fd2":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"cf7a9f2680db4e73b568b734bc06c96e":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"59ca467caa3a4f82840a33e5fbc51ce4":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0b7fea5795f642a89c5f7710532f4372":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"1f81fdf921ee475391808ba5fe3e8ae2":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_9ace3a7b08d34c72b58b78df1252d45d","IPY_MODEL_c24576641bba45789cd052c5573db176","IPY_MODEL_d14ba065e61b423d95812cdc5866e605"],"layout":"IPY_MODEL_806c1071c5e44f8c99439ad23406c0ca"}},"9ace3a7b08d34c72b58b78df1252d45d":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_fccd3bc99a5c4a0a8609943c4dceceab","placeholder":"​","style":"IPY_MODEL_45d5d3d67d8f4721b05ce3bb5e6d2bd6","value":"Downloading (…)in/label_encoder.txt: 100%"}},"c24576641bba45789cd052c5573db176":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_5334693c22aa42fbb30f096be91dbce9","max":128619,"min":0,"orientation":"horizontal","style":"IPY_MODEL_94b9110df3674736a61f67619c803be9","value":128619}},"d14ba065e61b423d95812cdc5866e605":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_f8ff5b4e316148f3aeaf3322ed44c0e8","placeholder":"​","style":"IPY_MODEL_a61b373ce80843c08ea39ca4001b38aa","value":" 129k/129k [00:00&lt;00:00, 369kB/s]"}},"806c1071c5e44f8c99439ad23406c0ca":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"fccd3bc99a5c4a0a8609943c4dceceab":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"45d5d3d67d8f4721b05ce3bb5e6d2bd6":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"5334693c22aa42fbb30f096be91dbce9":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"94b9110df3674736a61f67619c803be9":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"f8ff5b4e316148f3aeaf3322ed44c0e8":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a61b373ce80843c08ea39ca4001b38aa":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"badfe1e7892e4f6d98de990512cdd66a":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_3fd1a7bf2e384a9389a73ac7036fcdfc","IPY_MODEL_640664c83922472996b73d7c5649b56d","IPY_MODEL_0b43d3c702274b998de9a9cea487764e"],"layout":"IPY_MODEL_2d06872741424a568f78ffecdbfcb09f"}},"3fd1a7bf2e384a9389a73ac7036fcdfc":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_a2b6607e896c44dd85392da6603f17d1","placeholder":"​","style":"IPY_MODEL_dd9fbb00290a4b5a9ab4665c93062da1","value":"Map: 100%"}},"640664c83922472996b73d7c5649b56d":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"","description":"","description_tooltip":null,"layout":"IPY_MODEL_ae9eb8ca1c914d9c90e3408a6a342c8d","max":557,"min":0,"orientation":"horizontal","style":"IPY_MODEL_dc26466af71742908f7c78f7d452ddcb","value":557}},"0b43d3c702274b998de9a9cea487764e":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_4832fceac7f545a9a68a5f57ed31edc6","placeholder":"​","style":"IPY_MODEL_95782bc14f8041208e139eb1eb21d4fe","value":" 557/557 [01:49&lt;00:00,  6.53 examples/s]"}},"2d06872741424a568f78ffecdbfcb09f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a2b6607e896c44dd85392da6603f17d1":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"dd9fbb00290a4b5a9ab4665c93062da1":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"ae9eb8ca1c914d9c90e3408a6a342c8d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"dc26466af71742908f7c78f7d452ddcb":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"4832fceac7f545a9a68a5f57ed31edc6":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"95782bc14f8041208e139eb1eb21d4fe":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Wb8hgMoKBWk-","executionInfo":{"status":"ok","timestamp":1684885267433,"user_tz":420,"elapsed":21749,"user":{"displayName":"An Truong","userId":"05662548214724721674"}},"outputId":"5f58f8d3-dfab-4846-e01e-7e8189345eb5"},"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting datasets\n","  Downloading datasets-2.12.0-py3-none-any.whl (474 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m474.6/474.6 kB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: soundfile in /usr/local/lib/python3.10/dist-packages (0.12.1)\n","Collecting speechbrain\n","  Downloading speechbrain-0.5.14-py3-none-any.whl (519 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m519.0/519.0 kB\u001b[0m \u001b[31m46.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting num2words\n","  Downloading num2words-0.5.12-py3-none-any.whl (125 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m125.2/125.2 kB\u001b[0m \u001b[31m18.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.22.4)\n","Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (9.0.0)\n","Collecting dill<0.3.7,>=0.3.0 (from datasets)\n","  Downloading dill-0.3.6-py3-none-any.whl (110 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m110.5/110.5 kB\u001b[0m \u001b[31m14.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (1.5.3)\n","Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.27.1)\n","Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.65.0)\n","Collecting xxhash (from datasets)\n","  Downloading xxhash-3.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (212 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m212.5/212.5 kB\u001b[0m \u001b[31m25.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting multiprocess (from datasets)\n","  Downloading multiprocess-0.70.14-py310-none-any.whl (134 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.3/134.3 kB\u001b[0m \u001b[31m19.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: fsspec[http]>=2021.11.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (2023.4.0)\n","Collecting aiohttp (from datasets)\n","  Downloading aiohttp-3.8.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.0 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m66.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting huggingface-hub<1.0.0,>=0.11.0 (from datasets)\n","  Downloading huggingface_hub-0.14.1-py3-none-any.whl (224 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m224.5/224.5 kB\u001b[0m \u001b[31m29.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (23.1)\n","Collecting responses<0.19 (from datasets)\n","  Downloading responses-0.18.0-py3-none-any.whl (38 kB)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0)\n","Requirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.10/dist-packages (from soundfile) (1.15.1)\n","Collecting hyperpyyaml (from speechbrain)\n","  Downloading HyperPyYAML-1.2.0-py3-none-any.whl (16 kB)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from speechbrain) (1.2.0)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from speechbrain) (1.10.1)\n","Collecting sentencepiece (from speechbrain)\n","  Downloading sentencepiece-0.1.99-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m44.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: torch>=1.9 in /usr/local/lib/python3.10/dist-packages (from speechbrain) (2.0.1+cu118)\n","Requirement already satisfied: torchaudio in /usr/local/lib/python3.10/dist-packages (from speechbrain) (2.0.2+cu118)\n","Requirement already satisfied: librosa in /usr/local/lib/python3.10/dist-packages (from datasets) (0.10.0.post2)\n","Collecting docopt>=0.6.2 (from num2words)\n","  Downloading docopt-0.6.2.tar.gz (25 kB)\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.0->soundfile) (2.21)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (23.1.0)\n","Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (2.0.12)\n","Collecting multidict<7.0,>=4.5 (from aiohttp->datasets)\n","  Downloading multidict-6.0.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (114 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m114.5/114.5 kB\u001b[0m \u001b[31m12.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting async-timeout<5.0,>=4.0.0a3 (from aiohttp->datasets)\n","  Downloading async_timeout-4.0.2-py3-none-any.whl (5.8 kB)\n","Collecting yarl<2.0,>=1.0 (from aiohttp->datasets)\n","  Downloading yarl-1.9.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (268 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m268.8/268.8 kB\u001b[0m \u001b[31m28.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting frozenlist>=1.1.1 (from aiohttp->datasets)\n","  Downloading frozenlist-1.3.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (149 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m149.6/149.6 kB\u001b[0m \u001b[31m20.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting aiosignal>=1.1.2 (from aiohttp->datasets)\n","  Downloading aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0.0,>=0.11.0->datasets) (3.12.0)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0.0,>=0.11.0->datasets) (4.5.0)\n","Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (1.26.15)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2022.12.7)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (3.4)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.9->speechbrain) (1.11.1)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.9->speechbrain) (3.1)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.9->speechbrain) (3.1.2)\n","Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.9->speechbrain) (2.0.0)\n","Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.9->speechbrain) (3.25.2)\n","Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.9->speechbrain) (16.0.5)\n","Collecting ruamel.yaml>=0.17.8 (from hyperpyyaml->speechbrain)\n","  Downloading ruamel.yaml-0.17.26-py3-none-any.whl (109 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m109.1/109.1 kB\u001b[0m \u001b[31m15.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: audioread>=2.1.9 in /usr/local/lib/python3.10/dist-packages (from librosa->datasets) (3.0.0)\n","Requirement already satisfied: scikit-learn>=0.20.0 in /usr/local/lib/python3.10/dist-packages (from librosa->datasets) (1.2.2)\n","Requirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from librosa->datasets) (4.4.2)\n","Requirement already satisfied: numba>=0.51.0 in /usr/local/lib/python3.10/dist-packages (from librosa->datasets) (0.56.4)\n","Requirement already satisfied: pooch<1.7,>=1.0 in /usr/local/lib/python3.10/dist-packages (from librosa->datasets) (1.6.0)\n","Requirement already satisfied: soxr>=0.3.2 in /usr/local/lib/python3.10/dist-packages (from librosa->datasets) (0.3.5)\n","Requirement already satisfied: lazy-loader>=0.1 in /usr/local/lib/python3.10/dist-packages (from librosa->datasets) (0.2)\n","Requirement already satisfied: msgpack>=1.0 in /usr/local/lib/python3.10/dist-packages (from librosa->datasets) (1.0.5)\n","Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2022.7.1)\n","Requirement already satisfied: llvmlite<0.40,>=0.39.0dev0 in /usr/local/lib/python3.10/dist-packages (from numba>=0.51.0->librosa->datasets) (0.39.1)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from numba>=0.51.0->librosa->datasets) (67.7.2)\n","Requirement already satisfied: appdirs>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from pooch<1.7,>=1.0->librosa->datasets) (1.4.4)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->datasets) (1.16.0)\n","Collecting ruamel.yaml.clib>=0.2.7 (from ruamel.yaml>=0.17.8->hyperpyyaml->speechbrain)\n","  Downloading ruamel.yaml.clib-0.2.7-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.manylinux_2_24_x86_64.whl (485 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m485.6/485.6 kB\u001b[0m \u001b[31m53.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.20.0->librosa->datasets) (3.1.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.9->speechbrain) (2.1.2)\n","Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.9->speechbrain) (1.3.0)\n","Building wheels for collected packages: docopt\n","  Building wheel for docopt (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for docopt: filename=docopt-0.6.2-py2.py3-none-any.whl size=13707 sha256=9d058cf14d4f2e3d57596e9f306ee4544240a8b1cdc453883bf455082447bbea\n","  Stored in directory: /root/.cache/pip/wheels/fc/ab/d4/5da2067ac95b36618c629a5f93f809425700506f72c9732fac\n","Successfully built docopt\n","Installing collected packages: sentencepiece, docopt, xxhash, ruamel.yaml.clib, num2words, multidict, frozenlist, dill, async-timeout, yarl, ruamel.yaml, responses, multiprocess, huggingface-hub, aiosignal, hyperpyyaml, aiohttp, datasets, speechbrain\n","Successfully installed aiohttp-3.8.4 aiosignal-1.3.1 async-timeout-4.0.2 datasets-2.12.0 dill-0.3.6 docopt-0.6.2 frozenlist-1.3.3 huggingface-hub-0.14.1 hyperpyyaml-1.2.0 multidict-6.0.4 multiprocess-0.70.14 num2words-0.5.12 responses-0.18.0 ruamel.yaml-0.17.26 ruamel.yaml.clib-0.2.7 sentencepiece-0.1.99 speechbrain-0.5.14 xxhash-3.2.0 yarl-1.9.2\n"]}],"source":["!pip install datasets soundfile speechbrain datasets[audio] SpeechBrain num2words"]},{"cell_type":"code","source":["!pip install git+https://github.com/huggingface/transformers.git"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Sz5HxTIYBea6","executionInfo":{"status":"ok","timestamp":1684885308272,"user_tz":420,"elapsed":40843,"user":{"displayName":"An Truong","userId":"05662548214724721674"}},"outputId":"ee7f908d-7a6d-459b-8047-fd920515c1f9"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting git+https://github.com/huggingface/transformers.git\n","  Cloning https://github.com/huggingface/transformers.git to /tmp/pip-req-build-s3lnba_n\n","  Running command git clone --filter=blob:none --quiet https://github.com/huggingface/transformers.git /tmp/pip-req-build-s3lnba_n\n","  Resolved https://github.com/huggingface/transformers.git to commit 003a0cf8cc4d78e47ef9debfb1e93a5c1197ca9a\n","  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n","  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n","  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers==4.30.0.dev0) (3.12.0)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.14.1 in /usr/local/lib/python3.10/dist-packages (from transformers==4.30.0.dev0) (0.14.1)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers==4.30.0.dev0) (1.22.4)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers==4.30.0.dev0) (23.1)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers==4.30.0.dev0) (6.0)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers==4.30.0.dev0) (2022.10.31)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers==4.30.0.dev0) (2.27.1)\n","Collecting tokenizers!=0.11.3,<0.14,>=0.11.1 (from transformers==4.30.0.dev0)\n","  Downloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m55.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting safetensors>=0.2.1 (from transformers==4.30.0.dev0)\n","  Downloading safetensors-0.3.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m87.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers==4.30.0.dev0) (4.65.0)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers==4.30.0.dev0) (2023.4.0)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers==4.30.0.dev0) (4.5.0)\n","Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.30.0.dev0) (1.26.15)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.30.0.dev0) (2022.12.7)\n","Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.30.0.dev0) (2.0.12)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.30.0.dev0) (3.4)\n","Building wheels for collected packages: transformers\n","  Building wheel for transformers (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for transformers: filename=transformers-4.30.0.dev0-py3-none-any.whl size=7123844 sha256=e45d9050850d018736f1b1a9906227ba09e898076cbc67c1ea733bee164570e9\n","  Stored in directory: /tmp/pip-ephem-wheel-cache-ap2k1ern/wheels/e7/9c/5b/e1a9c8007c343041e61cc484433d512ea9274272e3fcbe7c16\n","Successfully built transformers\n","Installing collected packages: tokenizers, safetensors, transformers\n","Successfully installed safetensors-0.3.1 tokenizers-0.13.3 transformers-4.30.0.dev0\n"]}]},{"cell_type":"code","source":["import torch\n","device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\""],"metadata":{"id":"pc6kzmicBiB6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from transformers import SpeechT5Processor, SpeechT5ForTextToSpeech\n","\n","processor = SpeechT5Processor.from_pretrained(\"microsoft/speecht5_tts\")\n","model = SpeechT5ForTextToSpeech.from_pretrained(\"microsoft/speecht5_tts\")"],"metadata":{"id":"mhRGR28-Bnr4","colab":{"base_uri":"https://localhost:8080/","height":241,"referenced_widgets":["b373a18f432541eeaa4b94879a3a8241","62929bf449c74270a916b3e0243c8241","fb972eae15104b728d3ec3aed8cb5774","873b15f41f5745f8bc6a10589b88044f","45fd6e32afb04857b2227990a478d37f","c2b113b5b9bf455693fa8b095e6eac7e","de32588a0f724112aa2e09920deca398","65676abcb9bb4e239386921c37ee72a4","bbcec16c89b24f09bdfd918b145ecadd","a84bae4accbe451c89681141362e25e7","943c016e142d42a09ecd2e96d73a4d54","88d92b6abc5b4e52b04d81ef2e3fad57","d1f6924fab05495d8a6e574012e74d1f","c707815002b944aca410092614745b6f","077033813d904f3abeae9a704c37fd77","54949c4f1d9544b8adf67c405e1ea83c","109e5aaf06f84cb195e269291d872c83","1cbe285c8af747d29a9d66590229b0ca","6936f4fe05234828817f91678790ff0e","0f50c505eb894c718bef0f4598507cd7","027e0816746e4507b868081f21979d98","566b01ddf8ee460b9262949a443fd814","0ee95be406ff4b55bf8815df9acfab7b","32a9607aa01843e097eb161a53cfd708","92d2b819f9b04c389f11ac9d53b54250","5da911c12f004b1fb0fb84bb14caae2b","13df5f534a4f414c8cbe3e58cb6850a5","ab11520441c24c32b8f47e66bb834d2b","885078e7cb684550b1e530f6df85897b","51db7180036546cd8f5b59f27d72a944","9dbf17817d5846ddbb1b74b5950a022e","5638c69d52d64da784f090b3fecb69c3","bda4745e2be44d15b3fb9848dabea833","8c00e3f928724cfdbc2229feec283894","982c880b765d452ba5ebf6949d4cf782","971ee60309bf4fd88832a93d0f0b7588","fa1ed0d3bfec41e79cf21ed70751d5e4","a4d00cf619e04350b885864ac0f74956","674e2c0143e04e45a99402b91dd5ccbf","527a02c531e342d5b5c581b8ceb368d1","b4986b132106477c930a80bf9994482e","97373a161f70467499cb572f199327aa","fa09cc15bd934173abd11dda35d0adc8","f38819aeb9c24017886ef900cbbcc499","715b0ca6dcea4800af4d71fb45b812d2","c3f5b16bb4cd451ba632760bdaae847e","757a1f6210374ff1a6ad15fef45807a1","6fb7d8b3eb1b43f9ac4dc65d0fe763b8","48099cf01e8e4ccebd298e526cd8c159","f5b7074978074a6c8a5400a1e373975b","1702de65d6584a558e885e805ced5b24","2d18dc538ee44f81ba32593bc39b8ad2","bc030b008696413c97821a26620627c6","fb168617ce324fb1a7f49e9d7431837a","097cdf22fbdf4625960cb2de224cfb0c","c75d3ac02ebb4a7fb79a28d737b09eec","ac293583355f4b1da9e11dc6c8c1a567","ccf3da853c114c68b7c8e6a95c8e38d4","f91c845cf3ca46029021f6b928497718","7ad76d011036402faebc8948509d3c3b","2ad3404544244e7e965c1137ed38ae6d","4fdcbe20fa604b399bcc2758141c168f","8e7f888f480f4cb1a47f9b42af83a31a","83dd9d35b89044cb97c3f8f79826ee69","d2c009baa5cc4a199a41338482aaa8d9","40c2508b39f54fd5881cc863c13e7ad0","9c55bafb27d4411fab0155638ac69194","b1ef4cf6b3dd46cfaf66b255dccf01b2","b8aa27b690b0414f8506129a943b41ab","2498039a46e0485c8d5815b37c25694b","5010a0ecd9594bcebf563c0a23bdfb09","21f0ec50791a401a9384cebb95e945cb","c82e6547051044e0ab8df397d2ca7d42","7ab8f31a9e5146a594223f06a7e4343d","32511e7a13f344c595e1aac9684c3391","7afe905fb88c433ba811c8f6455a5576","68ac5d1b71ca4b7aac2ed33d4e97a062"]},"executionInfo":{"status":"ok","timestamp":1684885348457,"user_tz":420,"elapsed":36637,"user":{"displayName":"An Truong","userId":"05662548214724721674"}},"outputId":"8deb6875-1218-43ec-906c-1d31bf4dff9c"},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":["Downloading (…)rocessor_config.json:   0%|          | 0.00/433 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b373a18f432541eeaa4b94879a3a8241"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Downloading spm_char.model:   0%|          | 0.00/238k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"88d92b6abc5b4e52b04d81ef2e3fad57"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Downloading (…)in/added_tokens.json:   0%|          | 0.00/40.0 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0ee95be406ff4b55bf8815df9acfab7b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Downloading (…)cial_tokens_map.json:   0%|          | 0.00/234 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8c00e3f928724cfdbc2229feec283894"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Downloading (…)okenizer_config.json:   0%|          | 0.00/232 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"715b0ca6dcea4800af4d71fb45b812d2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Downloading (…)lve/main/config.json:   0%|          | 0.00/2.06k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c75d3ac02ebb4a7fb79a28d737b09eec"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Downloading pytorch_model.bin:   0%|          | 0.00/585M [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9c55bafb27d4411fab0155638ac69194"}},"metadata":{}}]},{"cell_type":"code","source":["model.to(device)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"NK6lEadhBxFu","executionInfo":{"status":"ok","timestamp":1684885353841,"user_tz":420,"elapsed":5399,"user":{"displayName":"An Truong","userId":"05662548214724721674"}},"outputId":"669ea4d1-694b-4db5-9869-a9ecd7ac2eed"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["SpeechT5ForTextToSpeech(\n","  (speecht5): SpeechT5Model(\n","    (encoder): SpeechT5EncoderWithTextPrenet(\n","      (prenet): SpeechT5TextEncoderPrenet(\n","        (embed_tokens): Embedding(81, 768, padding_idx=1)\n","        (encode_positions): SpeechT5ScaledPositionalEncoding(\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","      )\n","      (wrapped_encoder): SpeechT5Encoder(\n","        (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","        (dropout): Dropout(p=0.1, inplace=False)\n","        (layers): ModuleList(\n","          (0-11): 12 x SpeechT5EncoderLayer(\n","            (attention): SpeechT5Attention(\n","              (k_proj): Linear(in_features=768, out_features=768, bias=True)\n","              (v_proj): Linear(in_features=768, out_features=768, bias=True)\n","              (q_proj): Linear(in_features=768, out_features=768, bias=True)\n","              (out_proj): Linear(in_features=768, out_features=768, bias=True)\n","            )\n","            (dropout): Dropout(p=0.1, inplace=False)\n","            (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","            (feed_forward): SpeechT5FeedForward(\n","              (intermediate_dropout): Dropout(p=0.1, inplace=False)\n","              (intermediate_dense): Linear(in_features=768, out_features=3072, bias=True)\n","              (intermediate_act_fn): GELUActivation()\n","              (output_dense): Linear(in_features=3072, out_features=768, bias=True)\n","              (output_dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","          )\n","        )\n","        (embed_positions): SpeechT5RelativePositionalEncoding(\n","          (pe_k): Embedding(320, 64)\n","        )\n","      )\n","    )\n","    (decoder): SpeechT5DecoderWithSpeechPrenet(\n","      (prenet): SpeechT5SpeechDecoderPrenet(\n","        (layers): ModuleList(\n","          (0): Linear(in_features=80, out_features=256, bias=True)\n","          (1): Linear(in_features=256, out_features=256, bias=True)\n","        )\n","        (final_layer): Linear(in_features=256, out_features=768, bias=True)\n","        (encode_positions): SpeechT5ScaledPositionalEncoding(\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","        (speaker_embeds_layer): Linear(in_features=1280, out_features=768, bias=True)\n","      )\n","      (wrapped_decoder): SpeechT5Decoder(\n","        (layers): ModuleList(\n","          (0-5): 6 x SpeechT5DecoderLayer(\n","            (self_attn): SpeechT5Attention(\n","              (k_proj): Linear(in_features=768, out_features=768, bias=True)\n","              (v_proj): Linear(in_features=768, out_features=768, bias=True)\n","              (q_proj): Linear(in_features=768, out_features=768, bias=True)\n","              (out_proj): Linear(in_features=768, out_features=768, bias=True)\n","            )\n","            (dropout): Dropout(p=0.1, inplace=False)\n","            (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","            (encoder_attn): SpeechT5Attention(\n","              (k_proj): Linear(in_features=768, out_features=768, bias=True)\n","              (v_proj): Linear(in_features=768, out_features=768, bias=True)\n","              (q_proj): Linear(in_features=768, out_features=768, bias=True)\n","              (out_proj): Linear(in_features=768, out_features=768, bias=True)\n","            )\n","            (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","            (feed_forward): SpeechT5FeedForward(\n","              (intermediate_dropout): Dropout(p=0.1, inplace=False)\n","              (intermediate_dense): Linear(in_features=768, out_features=3072, bias=True)\n","              (intermediate_act_fn): GELUActivation()\n","              (output_dense): Linear(in_features=3072, out_features=768, bias=True)\n","              (output_dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","          )\n","        )\n","      )\n","    )\n","  )\n","  (speech_decoder_postnet): SpeechT5SpeechDecoderPostnet(\n","    (feat_out): Linear(in_features=768, out_features=160, bias=True)\n","    (prob_out): Linear(in_features=768, out_features=2, bias=True)\n","    (layers): ModuleList(\n","      (0): SpeechT5BatchNormConvLayer(\n","        (conv): Conv1d(80, 256, kernel_size=(5,), stride=(1,), padding=(2,), bias=False)\n","        (batch_norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (activation): Tanh()\n","        (dropout): Dropout(p=0.5, inplace=False)\n","      )\n","      (1-3): 3 x SpeechT5BatchNormConvLayer(\n","        (conv): Conv1d(256, 256, kernel_size=(5,), stride=(1,), padding=(2,), bias=False)\n","        (batch_norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (activation): Tanh()\n","        (dropout): Dropout(p=0.5, inplace=False)\n","      )\n","      (4): SpeechT5BatchNormConvLayer(\n","        (conv): Conv1d(256, 80, kernel_size=(5,), stride=(1,), padding=(2,), bias=False)\n","        (batch_norm): BatchNorm1d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (dropout): Dropout(p=0.5, inplace=False)\n","      )\n","    )\n","  )\n",")"]},"metadata":{},"execution_count":5}]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"xs0TmtaqCBV3","executionInfo":{"status":"ok","timestamp":1684885380039,"user_tz":420,"elapsed":26213,"user":{"displayName":"An Truong","userId":"05662548214724721674"}},"outputId":"b99c3bef-90dc-4f97-d1f4-97d5d5013767"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","source":["%cd /content/drive/MyDrive/cs_172b_project"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"lLQONP7sCC-S","executionInfo":{"status":"ok","timestamp":1684885380040,"user_tz":420,"elapsed":7,"user":{"displayName":"An Truong","userId":"05662548214724721674"}},"outputId":"037e9613-680a-42c5-de8d-338439aafa53"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/.shortcut-targets-by-id/1N_x4A52JJZ1qrsAemqV40plE7rZS_FTL/cs_172b_project\n"]}]},{"cell_type":"code","source":["from datasets import load_dataset, Audio\n","dataset = load_dataset(\"audiofolder\", data_dir=\"full_pierre_dataset/\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":233,"referenced_widgets":["2952a39ee3f04d378b492c2d312b64d2","08186d80349b463cbe067838882c3022","beda7be92eec4a4693caeeafb14162d5","8daa3dd103d54ae48dcd6db67f9fa7a3","469f5749d0cd44a597458b7ab135a556","666c452bb5cd48ba833d48ef581319bb","617a33922ea9446582f89d391b2a5fb4","cc0354c0f8b44776ac0417bd0f4191b0","28246d1f9ccb440ba532d63b350deeb0","21c376345c1047329b38e69631e1250d","b99f824c5b6f4211848569a995dd475a","d3a43708fa88406aa8188b33845df847","8ca48ed597c84d4f8f2eb4772b4ba6f2","5fe04d4d9fbc49d7a5496cbce4e62912","ee5c5852c0ef468c861d1d7c6ecfc548","1504d8848e074d23899b996a7e3359bc","a52a49a08248436082d7546d8ab50a8d","9ee7ac5d7b86420e95cf563e8a540fad","f9e9ed2525d44563a75408aa0c3f57e3","e18b544754e14aada41ae7f8a5e50962","1eef9bbe6dba447aa91c66d0a2418ff8","3080ba74016a43f996ea766588fa8f92","5add7f86eede40bf9e02a188c9b832f4","e7e23af165fb4696838eb088a4ea0d41","09bc2fbad114428e9572a09b3679fdf5","c1ac29e03de64904932c2fa5dab26b8e","ce61f7fa8e8f4bb29374b84aa7beac8c","7821e578a7cc48ec9fa2af276210813d","89585dc820c7452283e6cb54be9efa73","a87a94f4958d44589870594e5e72476d","f62da19f8425497e82d129228f14ccaf","d7450e3728504ff1bd3fa4277a0a1b81","384d1c1be891428f879e147e16a991d3","f1cae5f4dacb41298c4ca0a212355fd1","a3bf51a9e1284a8f83ef19b09d3c7d0d","3129b8b060564a1c83656c3b263c3a94","913e407dd3844f289ed07448d90cf422","1c226373f546484ca15f603d9095447f","84f69df8a8f943ac89201da0cef30ddf","4c5096fb48a84eaa9bc4044cd71098a8","b132e34445c44bb3b426d6be6bd32e3b","f9602086721143e8a3f5a615c557d72b","cf10c2b66eec4d8eac67dd44470af183","a0eb47bf6ae64b57aa13cb05ac417386","dbbbba84fa424ae38f4c80eaa13a7c2b","11d3a4b025f342138c616f2f31df1aad","ea30b1b190e145fa89a94560a915a68c","5bab135460ee4723829de34f5e4cffaa","7fea282189cc426f92cab2b4d7aa683e","1fe418d5bfff4fddbb87416fd39b3a9d","efd32706490f4172846f491dfcfaea71","55e5447c14904c8286417d6f99377d65","9f9d1823475f4d73a0b20ef615320fdb","9e046391c61142acb44f2486bfc3c83c","9e3d78978f3f43bfb680591b7ea7cbe2","3d0f69de10be4bdb846c692a912bd78d","9d679e4e80b94f9c90dafc40faa662ed","89b95ba68c5b4b4f9c5d8b39452a6bcc","42a35630246846d7bb5e76758e01457e","5c4f9929950e4b73b7d06107218eef57","dd93a5f01c57492789e969b4b1ba7684","b55ea99aca5c4390831d621dbbdb94ce","6aa0627fceef442395bc52a0e5ecbbf4","8d240f5e8daf464f8a2530349d401f13","3912ab8b602e4ce486de8a6bbc575760","ada621d9abd84815bfc8f46f4936e7f7"]},"id":"oPjHvW-RCHf5","executionInfo":{"status":"ok","timestamp":1684885394167,"user_tz":420,"elapsed":14131,"user":{"displayName":"An Truong","userId":"05662548214724721674"}},"outputId":"79a18d0b-7539-46a5-f13c-658cac4d904b"},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":["Resolving data files:   0%|          | 0/558 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2952a39ee3f04d378b492c2d312b64d2"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Downloading and preparing dataset audiofolder/default to /root/.cache/huggingface/datasets/audiofolder/default-fbcc41d2958a7db5/0.0.0/6cbdd16f8688354c63b4e2a36e1585d05de285023ee6443ffd71c4182055c0fc...\n"]},{"output_type":"display_data","data":{"text/plain":["Downloading data files:   0%|          | 0/558 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d3a43708fa88406aa8188b33845df847"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Downloading data files: 0it [00:00, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5add7f86eede40bf9e02a188c9b832f4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Extracting data files: 0it [00:00, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f1cae5f4dacb41298c4ca0a212355fd1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Generating train split: 0 examples [00:00, ? examples/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"dbbbba84fa424ae38f4c80eaa13a7c2b"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Dataset audiofolder downloaded and prepared to /root/.cache/huggingface/datasets/audiofolder/default-fbcc41d2958a7db5/0.0.0/6cbdd16f8688354c63b4e2a36e1585d05de285023ee6443ffd71c4182055c0fc. Subsequent calls will reuse this data.\n"]},{"output_type":"display_data","data":{"text/plain":["  0%|          | 0/1 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3d0f69de10be4bdb846c692a912bd78d"}},"metadata":{}}]},{"cell_type":"code","source":["# set sampling rate of audio data to 16 kHz for SpeechT5\n","dataset = dataset.cast_column(\"audio\", Audio(sampling_rate=16000))"],"metadata":{"id":"CCfQq9McCJiW"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["dataset['train']"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Zjy7J9NDFMtM","executionInfo":{"status":"ok","timestamp":1684885394175,"user_tz":420,"elapsed":68,"user":{"displayName":"An Truong","userId":"05662548214724721674"}},"outputId":"01173159-0d6e-45aa-ba0e-9df682eab430"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["Dataset({\n","    features: ['audio', 'transcription'],\n","    num_rows: 557\n","})"]},"metadata":{},"execution_count":10}]},{"cell_type":"markdown","source":["# Clean up text"],"metadata":{"id":"-kZ89qA0CgBd"}},{"cell_type":"code","source":["tokenizer = processor.tokenizer"],"metadata":{"id":"uy0qOEWfCiSB"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import re\n","from num2words import num2words\n","\n","def replace_matched_pattern(match):\n","    pattern = match.group()\n","    if re.match(r'[a-zA-Z]\\d+', pattern):\n","        # if pattern is something like 'V1', 'X2', etc.\n","        number_part = re.search(r'\\d+', pattern).group()  # get the number part\n","        letter_part = re.search(r'[a-zA-Z]', pattern).group()  # get the letter part\n","        return letter_part + \" \" + num2words(int(number_part))\n","    elif re.match(r'\\d+[a-zA-Z]', pattern):\n","        # if pattern is something like '2n', '3m', etc.\n","        number_part = re.search(r'\\d+', pattern).group()  # get the number part\n","        letter_part = re.search(r'[a-zA-Z]', pattern).group()  # get the letter part\n","        return num2words(int(number_part)) + \" \" + letter_part\n","    elif pattern.endswith('s') and len(pattern) == 3:\n","        # if pattern is a decade like '80s', '90s', etc.\n","        number_part = pattern[:-1]  # get the number part\n","        return num2words(int(number_part), to='year') + \"s\"\n","    elif pattern.endswith('s') and len(pattern) > 3:\n","        # if pattern is a century like '1800s', '1900s', etc.\n","        number_part = pattern[:-1]  # get the number part\n","        return num2words(int(number_part), to='year') + \"s\"\n","    else:\n","        # default case for standalone numbers\n","        return num2words(int(pattern))\n","\n","def cleanup_text(inputs):\n","    text = inputs[\"transcription\"]\n","    # adjust the regular expression to match 'V1', 'X2', etc., '2n', '3m', etc., and '80s', '90s', etc.\n","    text_with_numbers_replaced = re.sub(r'\\b([a-zA-Z]\\d+|\\d+[a-zA-Z]|\\d+s|\\d+)\\b', replace_matched_pattern, text)\n","    inputs[\"transcription\"] = text_with_numbers_replaced\n","    return inputs\n","\n","dataset = dataset.map(cleanup_text)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":17,"referenced_widgets":["f59174495a8f4500a485f6f3414018bd","9c23888e2cf84195a50f51bb29334c7f","c9f751993e3d4b7ea2e63a6b016da812","0a998db71e264187bd11af34da1db511","e31bb7e85f3043e9bb403e5ac1dcd916","c89edd18b12e49928c3bf57bd89ef4f8","a418823a9e4f4d99b7c0ce0f39fd69e8","ac23235623ad4f039c742422dcf027c8","8079a9c2f66441ba8fcf0452c2b4d02f","d5ec8ab7f8fb47de89e3eba945adde4b","98297d9b88dc48abb6df2f6ad561e5e0"]},"id":"xWPkfyL3CjX0","executionInfo":{"status":"ok","timestamp":1684885394182,"user_tz":420,"elapsed":73,"user":{"displayName":"An Truong","userId":"05662548214724721674"}},"outputId":"49f313de-c9e4-4be6-d352-9e5e530a3729"},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":["Map:   0%|          | 0/557 [00:00<?, ? examples/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f59174495a8f4500a485f6f3414018bd"}},"metadata":{}}]},{"cell_type":"code","source":["def extract_all_chars(batch):\n","    all_text = \" \".join(batch[\"transcription\"])\n","    print(all_text)\n","    vocab = list(set(all_text))\n","    return {\"vocab\": vocab}\n","\n","vocabs = dataset['train'].map(\n","    extract_all_chars, \n","    batched=True, \n","    batch_size=-1, \n","    keep_in_memory=True,\n","    remove_columns=dataset['train'].column_names,\n",")\n","\n","# Flattening the list of vocabs\n","flat_vocab_list = [item for sublist in vocabs[\"vocab\"] for item in sublist]\n","dataset_vocab = set(flat_vocab_list)\n","tokenizer_vocab = {k for k,_ in tokenizer.get_vocab().items()}\n","print(dataset_vocab)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":73,"referenced_widgets":["a49a65b432a6481081537d782b58e8f4","8debf062cb264e389cc04a8cb812b507","e65aa4c7f19a48c7865c20b88039f3ee","62c9bf462df0452696a79cd57170ea7b","39729571189540069f0308ba54d7b9b1","7bcb1e55f3eb4f57b07f0c720d6d18d7","92f3ab6a7638446f97ae464b6ce4886b","53f3e27f8a994852b8b54aea52ea5c99","c2ae062f965e466cbd5aee10d3f63917","99630af0117e442aa2b0841db651a303","95f000daa78643729ad8e7aa681afef6"]},"id":"0b2Sx-KfCn5p","executionInfo":{"status":"ok","timestamp":1684885394183,"user_tz":420,"elapsed":68,"user":{"displayName":"An Truong","userId":"05662548214724721674"}},"outputId":"7611bb98-0e1b-4920-bab4-bf79ade9ceb1"},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":["Map:   0%|          | 0/557 [00:00<?, ? examples/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a49a65b432a6481081537d782b58e8f4"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":[" for instance, right? So there has to be a channel, in a physical system, in the machine, there has to be a channel that conveys information about the targets all the way down to the  the targets all the way down to the deep weights. There is no other way, otherwise you cannot solve those critical equations and have the hidden weights depend on the target.  depends on the target. So there has to be this deep channel that conveys information about the targets and other things all the way from let's say the  other things, all the way from, let's say, the output back to the deep weights in a physical system, in your digital simulation fantasy, you don't worry.  You know, simulation fantasy, you don't worry about it, but in a physical system you have to worry about it. And so we can ask, for instance, where is the channel located? What kind of information does it carry?  What kind of information does it carry or what is the minimal amount of information that it has to carry and what is the rate of the channel etc. All classical Shannon theory  the channel, et cetera, all classical Shannon theory of communication can be applied to this object. So if you think about the brain, if you think that, you know, you're going to be able to  brain if you think that supervised learning is sort of a reasonable approximation to some form of biological learnings, what I'm telling you is that there has to be a channel  you is that there has to be a channel that goes all the way from the motor output or from wherever error functions are computed all the way back to each one of these sinuses. Otherwise,  each one of these sinapses. Otherwise, they cannot learn. And this is important also because it shows  Einstein. If you walk a few hundred meters from here there is Humbald University and that's where Einstein spends some time. In fact if you walk across the street I recommend you go  important also because it shows that the notion of feedback, the word feedback, has actually two completely different meanings that have to be separated. There is one type of feedback.  be separated. There is one type of feedback that is relatively fast that may occur, let's say in biological neurons on a scale of ten to one hundred milliseconds.  on a scale of ten to one hundred milliseconds. For instance, the visual system, we often talk about feedback where you have a bottom-up sensory stream that meets  have a bottom up sensory stream that meets a top down expectation or modeling stream. And together these two stream combines to stabilize your  to stabilize your perception, your percept of the world right. That's the kind of feedback that is sort of dynamical and very fast, again on the scale of ten to one hundred milliseconds.  on the scale of ten to one hundred milliseconds. Here we're talking about deep learning feedback, the feedback for learning, which can be much slower, could be hours,  which can be much slower, could be hours, could be days, and who'd perhaps use the same pathways, but also possibly completely different pathways.  completely different pathways. We definitely know that in the brain you have tons of connections going in the feedback direction. But again, two different possible notions of feedback.  two different possible notions of feedback. So let's discuss first where can this channel be in different neural systems.  this channel B in different neural systems. Again, you see immediately that there is a number of possibilities. You could have a system where signals can travel in both directions along axons.  walk across the street, I recommend you go see there is a glass plate on the pavement through which you can see some bookshelves and some books.  in both directions along axons, right? You can have systems where you have separate connections, but they are sort of identical. You use the transport.  are sort of identical, you use the transpose of the forward weights in the reverse. This is sort of what you think when you're doing your digital fantasy.  when you're doing your digital fantasy in your computer. That's the way you think about it. If you think about biology, many have pointed out it's very unlikely that you're going to be able to do it.  have pointed out, it's very unlikely that you have exact weights in the opposite direction. You could have a twin situation where you have a deep learning channel that has the same architecture.  channel that has the same architecture as the forward but is completely separate but with the same architecture. And then in my opinion the most plausible one is that you have  The most plausible one is that you have a completely distinct channel that has different architecture and different numbers of neurons, etc. But of course, which talks to the forward path.  which talks to the forward pathway and through different sets of weights and and that's what I call the distinct case. But for any  the distinct case. But for any physical system, you can think about these different possibilities and what happens with learning, with rates, et cetera.  with learning, with rates, et cetera, in these different scenarios. Now one important result that was obtained  One important result that was obtained by Lily Crapidal was that if you put completely random weights on the way back, back propagation still works.  see some bookshelves and books. This is where the Nazi burnt in one thousand, nine hundred and thirty-three. Burnt a lot of books from by Einstein and many other great Jewish people.  way back, back propagation still works, which is somewhat amazing. You don't need to have the transpose of the forward weight on the deep learning channel.  on the deep learning channel, you can have completely random matrices. And if you do the simulation, you see that it works almost as well as plain.  it works almost as well as playing back propagation. So that's a remarkable result. And it opens the door for doing a lot of studies on this day.  a lot of studies on these different architecture. So you can do studies on these different architectures using random weights on the backward path. But not only that, you can ask all kinds of questions.  But not only that, you can ask all kinds of questions. On the backward path, typically, you have a linear network. All the operations you do are matrix multiplication, you multiply maybe by two.  multiplication, you multiply maybe by the derivatives, etc. Why should the backward channel be linear and the forward path channel be non-linear? Right? You may want in a physical system use the same kind.  a physical system use the same kind of hardware in both directions so you may try to have non-linear neurons in the learning channel. You like to use  the learning channel. You like to use Dropout on the forward channel. Why not use Dropout on the backward channel? You like sparse matrices. Maybe we  If you like sparse matrices, maybe we want to use sparse matrices in the backward channel. Random sparse matrices. And what about these derivatives? Do you really?  What about these derivatives? Do you really need to multiply by the derivatives of the forward channel in the backward channel? Do you need all the derivatives, just the derivatives of the current layers?  and many other great Jewish scientists and pacifists and so on. But anyway, if you had met Einstein a few hundred meters from here,  And you adjust the weights in the forward channel by having back propagation become essentially the same once you have the backward learning channel. But why not adapt also the weights in the  But why not adapt also the weights in the backward channel? Maybe if they are made of the same hardware, you should be used HAB or back propagation in both channels in the forward channel and the...  channels in the forward channel and the deep learning channel. And if you look at this architecture, you can also have skipped connections like this. In fact, you can have an architecture that has only skipped.  can have an architecture that has only skip connections where basically you have connections running from the top layer back to all the different layers. So that's what we call a fully skip.  So that's what we call a fully skipped architectures. So you see that you end up with lots of possibilities and we've tried all of them essentially.  And we've tried all of them essentially, maybe a hundred or two hundred simulations of all the combinations. And the main result is that the deep learning channel is very robust. That is most of the things that we've been doing.  learning channel is very robust. That is most of these combinations they work. Or you can get them to work. They may be a little harder to get to work than plain back propagation,  done plain back propagation, but if you spend some time tweaking the learning rate, etc., you can get most of them to work. There is a couple of them that do not work. For instance, if you get read, you can get a lot of them to work.  do not work. For instance, if you get rid of all the derivatives, you don't multiply by any derivative in the deep learning channel, then you won't be able to learn well. You do need those derivatives.  learn well, you do need those derivatives. In fact, you can show even more than that. You only need the derivative of the current layers. You don't need the derivatives.  I'm Ed Einstein, a few hundred meters from here and you would ask him how did he come up with the theory of special relativity? Everyone knows that he would have said well, I just try to think that I was a ray of light  above the current layer like bag propagation does. Bag propagation uses all the derivatives above. You actually don't need all of them. If you have a skipped architecture, you can get rid of them.  have a skipped architecture, you can get rid of all those derivatives. But you will need the derivative of the current layer. And also another thing that is important that you see also in when you do the  that you see also in when you do the work of Lily Crombs, that the random weights start here, so they play a role in learning for the layer before the layer.  in learning for the layer before the last one, but the last layer is doing gradient descent. And that's absolutely necessary. If you remove the gradient descent from the top layer,  the gradient descent from the top layer, then all these algorithms start to break down. I can show you some example of simulation on NIST or C four, so these are  simulation on NIST or CFR. So these are example of this is back propagation, this is skipped back propagation, skipped random back propagation, random back propagation.  on back propagation, random back propagation, this is without the derivatives, etc. You can see that they all converge at different speeds, but they all converge. If you remove the derivatives, they're all gone.  If you remove the derivatives, these guys are not learning. This is the training set. This is the test set. This is another set of experiments where we are going to be doing the test set.  This is another set of experiments where we look at adaptive random backpropagation. So we're adapting the weights both on the forward channel and on the deep learning channel using  and on the deep learning channel using the same rule, product of the presynaptic time, post synaptic error if you want. So to speak, training set, as set,  of light, try to think how the world looks like if you are a photon. So here we're going to do the same thing. We're going to try to think that you're a neuron or try to  if you want. So to speak, training set, test set, you can see they all converge to good performance, et cetera, et cetera. These are experiment done with sparse run  experiment done with sparse random matrices with different levels of sparse city etc. Same thing. Same thing on C four. This is just to show you an interesting technique for some simulation that comes  interesting technique for some simulation that comes from paper by Sabatini et al. What you have at the top is your data. So there is a mathematical function that has a parameter K that allows you to  as a parameter k that allows you to adjust the complexity of the data. So if we Sk equal one, this is your data, k equal to it's like this, etc. So as k grows,  to it's like this, etc. So as K grows, you have more and more black dots in this picture, right? The picture becomes more and more complicated, it has to do with betting numbers. And basically the task of your network is you give it.  Basically, the task of your network is you give it as input two values x and y, and the output is the prediction, the classification, the two black and white. And these networks here have a hidden layer of the size.  And these networks here have a hidden layer of the size five hundred and there is maybe four or five hidden layers and Back propagation is here, you know is able to learn very well the patterns. This is the data. This  well the patterns, this is the data, this is what the backpropagation produces, and these are some of the other algorithms for instance random backpropagation, and you see that at low complexity it matches  And you see that at low complexity it matches the data and back propagation very well. At high complexity, at the same training, at the same epoch, it's  training at the same epoch, it's a little bit behind and there are variations across the different algorithms. This one doesn't seem to be doing very well, but you know, again,  that you're a neuron or try to think you are a synapse or an axon and ask yourself how would the world around you look like. So that's what we're going to talk about.  be doing very well. But again, tweaking, learning, grade, et cetera, you can get even this one after a while to look exactly like this. So now I want to tell you about what needs to be communicated.  about what needs to be communicated in this channel. And basically, you can see that you can reduce the amount of information that is needed.  of information that is needed. From the critical equation, you get the impression that you need to transmit the targets, the output, all the weights above, all the weights below, and all the weights below.  all the weights below and all the derivatives above. Back propagation alone already tells you that you only need in fact to send T- so you don't need to.  In fact, to send T minus O, you don't need to send TNO separately. You don't need all the weights below. In fact, the only thing you need is the activity of O.  The only thing you need is the activity of the pre-synaptic neurons. So all the information about all the weights below is subsumed by the  all the weights below is subsumed by the activity of the precinoptic neurons. This, by the way, suggests that it should be the same for all the weights above.  should be the same for all the weights above. You really don't need to know everything about all the weights above in the same way that you don't need to know about all the weights below.  about all the weights below. And this is exactly what leads and is shown by random back propagation because random back propagation has completely run  make propagation as completely random weights in the layer above to the layer that you're trying to learn. And then we skip the random back propagation.  you look like. So that's what we're going to do. And I'm going to first give you a few simple examples of this style of thinking for neurons, but the place where we're going to get, I think, something.  Then we skip the random back propagation. We see that we don't need all the derivatives of the layers above. You just need the derivative of the current layer. So basically what it seems,  So basically what it seems to work, the minimal amount of things that you need to send is some function of t minus o, a linear, a random value of t minus o,  function of D minus O, a linear, a random matrix of D minus O will work, a random sparse matrix will work. You can reduce the precision on D minus O quite a bit.  use the precision on t-minus so quite a bit, not down to one bit, if you do just one bit the sign of t-minus so it doesn't seem to work but a low precision version of t-minus so time the random sparse matrix will work.  plus the derivative of the current layer. You may guess that if you multiply this by a matrix, it would have to be a full-rank matrix, and it's five minutes long.  full rank matrix and it's five minutes left, it's about right but it's not a sharp threshold. If the random matrix is full rank, it works well. If it's one rank below the full rank, it's going to be a little bit more difficult.  If it's one rank below the full rank, it will still sort of learn. There will be graceful degradation in performance. They will not be echoed.  for imperformance, they will not be a complete collapse. So those are simulation results. Can we prove anything? Well, we can start with very simple networks.  And we can start with very simple networks. This is a simple linear network, for instance, with two layers, in this case three layers, you have weights A and B on the left.  layers you have weights A and B on the forward channel, weights C in the deep reverse channel and you can write down the equations of such a system in fact  we're going to get, I think, some interesting results is when we think about synapses. So here's a first example of this kind of thinking for a simple neuron. Imagine you're doing  down the equations of such a system. In fact, some of this is in the lily-crab paper. We have extended the theorem a little bit, but you can see that  a little bit, but you can see that the stable points are given by these hyperbolas here, where the products of the weights A and B is some constant, the right constant. And you can see it.  the right constant. And you can see in these phase spaces all these points are attractors, all the arrows are converging. If you start here you have a parabolic trajectory that ends up here.  the symbolic trajectory that ends up here. And here is the same thing except for this little piece of the Cypherbola, where actually the points are unstable. And the trajectory is tend to diverge from those points.  And the trajectory is tend to diverge from those points, but they will converge on some other points on those hyperboles. So for that very simple system, you can understand what happens. But of course,  can understand what happens. But of course, you would like to understand more complex system, even in the linear case, you could have multi-layer linear network. All these are matrices for one.  network, all these are matrices, forward matrices. You have random matrices on the way back, whether you do it in the skip fashion as drawn here or in the propagation fashion, it's essential.  in the propagation fraction, it's essentially equivalent. And if you write the learning equation for the matrices A, you get essentially this thing because it's  get essentially this thing because it's very easy to see that because it's just the transpose of the stream coming from the forward pathway which is  coming from the forward pathway, which is this object here. There is input transpose here. And then there is the feedback matrix CI times what is at the top.  for a simple neuron, imagine you're doing logistic regression. And I give you binary vectors to be classified into zero and ones. And the binary vector,  Thank you, Gregor, good morning. So we're going to talk about deep learning in the machine for lack of a better word. The machine, of course, is the brain or a neuromotor?  back matrix C i times the what is at the top t minus so when it combines with the input you get this quantity here. So this is the covariance matrix of the target  is the covariance matrix of the targets with respect to the inputs. P is the product of all the matrices and this is the covariance matrix of the input.  is the covariance matrix of the input data. If your backward pathway is also adaptive, you get these equations for adapting the C, so you get a huge system.  for adopting the C, so you get a huge system of differential equations. And just for comparison, these are the systems that you get for a back propagation. So can I?  to get for a back propagation. So can we solve such systems? Well, in general, no. These are actually very complicated systems. These are polynomial systems.  these are polynomial systems of differential equations. And if you are in one dimension, if you have d x, d t equal p of x, that we understand quite well.  of x, that we understand qualitatively quite well. I will show you an example at the very end, but basically you cannot have oscillations. You either have convergent to fixed points or divergence  to fix points or divergence to infinity. But if you have two differential equations, the x, dt equal p of x, y, dy, dt equal q of x, y, just  of x, y, dy, dt equal q of x, y, just two equations with superinomial that is extremely difficult. In fact, Hilbert's sixteen problem is the question of whether you can bound  the question of whether you can bound the number of limits cycle of such a system. And that question is completely unsolved. Steve's mail wrote a paper fifteen years ago, the new version of Hilbert's problem.  into zero and ones. And the binary vectors that should be classified into ones are those that are connected, that is, where all the zeros are together and all the ones are together. We're up around or not.  years ago, the new version of Hilbert's problem for the 21st century, it's in there and basically no progress has been made on this question for many decades. It's a little bit like a P equal NP.  it's a little bit like a P equal NP. So these are problems are very difficult, but in some restricted cases, we can solve them. For instance, if you have a long chain of a very deep chain of units that are all.  of very deep chain of units that are all linear. I'll show you this at the very end. If you have a system with one unit and units in the hidden layer, then one  unit and units in the even layer than one unit. This guy here we can solve, etc. There are interesting connections to complex ideas in algebraic geometry.  to complex ideas in algebraic geometry, like COZO complex, I won't go into that. And in some cases, you can solve the non-linear case, for instance, if you have a  the non-linear case, for instance, if you have the case with three units like this, where you put a non-linearity here like a power function, for instance. That case we can solve and show that it's going to be a problem.  that case we can solve and show that it converge. So just to finish, let me show you what happens in this case. So these two problems are equivalent again, the skipped version versus the...  the skip version versus the propagated version are equivalent. So let's stick with the skip version. What you have is a chain of linear neurons. Everything is linear.  neurons, everything is linear. If I give you an input, it gets multiplied by A one, A two, AL. So the output is A one, A two, AL time the input. Let's call P the product.  time the input. Let's call P the product of all these weights. Obviously you're trying to learn what's the right combination of weights, so you're adjusting a line and very simple problems.  the ones are together with wrap around or not that's a detail. So that's a task and I ask you, is this simple? Is it easy? Is it linearly separable, et cetera? How does it compare to pairs?  you're adjusting a line and very simple probably it's even convex but each one of these little weights it's applying its own learning rule based on this rather than learning.  learning rule based on this random feedback way CL, these weights are fixed but completely random. So if you write the differential equation,  So if you write the differential equations, they look like this. That's a system of differential equations that is satisfied by this large set of weights. If you look carefully, you can see that any...  look carefully, you can see that any two consecutive equations are coupled in this way, which means that the evolution of ai plus one is a quadratic function of ai. So, a two is a quadratic function of ai plus one is a quadratic function of ai plus one is a quadratic function of ai.  So A two is a quadratic function of A one, A three is a quadratic function of A two, so it's a quadratic function of A one, etc., etc. So basically you can take  So basically you can take the equation of A one, you can write A one, there is the product P of all the AI, you replace each AI as its function of A one.  you replace each AI as its function of a one. It's a large polynomial of degree, of even degree, and you end up with an equation that looks like this.  up with an equation that looks like this, the DA1 dt equals a big polynomial of A one, and if you look carefully, the polynomial is of degree twelve minus one.  the polynomial is of degree twelve minus one. So it's an odd polynomial with an odd degree and a negative leading coefficient. So it is something that's going to be a negative.  So it is something that looks like this, right? And so if you start anywhere, suppose you start here.  approval, etc. How does it compare to parity, for instance? And if you're not used to this, you may look at this and you may think that this is a fairly simple  you start anywhere. Suppose you start here, it says the derivative is negative, so you're going to move like this and you're going to end up to this fixed point here. If you start here, the same thing.  If you start here, the same thing, the derivative is positive, so you're going to end up here. Now, if you start here, at minus infinity or your derivative is positive, then you're going to end up here.  at minus infinity or your derivative is positive so you're going to move and then up here and the same thing here if you start here your derivative is negative so you're going to move to the left.  and add up here. So no matter where you start, this system is convergent and will converge to the right solution. So although you have completely random weights in the feedback channel,  and I'm waiting in the feedback channel. Although this could have a million layers, by magic, this system will always converge to a correct solution. Okay, I'm out of the question.  converge to a correct solution. OK, I'm out of time, so let me summarize. Learning the machine looks at learning in physical neural system, not in the digital fantasy  system, not in the digital fantasy that we use. Thinking about synapses and their environment,  synapses and their environment leads to the notion of local learning, the rules for adjusting a synaptic weight should depend only on local variables. This leads to the notion of the system.  on local variables. This leads to the notion of deep learning channel. If you depend only on local variable and you have a feed forward network, you will never be able to learn anything interesting.  be able to learn anything interesting. You need a backward channel, the deep learning channel, to communicate information about the targets, although it's not.  may think that this is a fairly simple task compared to parity because parity you have to look at all the bits and count whether there is an even or odd number of such bits.  the deep weights. The deep learning channel appears to be in simulation very robust to all kinds of perturbation and variations of the process.  perturbation and variations in algorithms, entopology, etc., etc. We have sort of corner what is the minimal information that is required to ensure  more information that is required to enable learning in deep synapses. And in some cases we can build a mathematical theory, but ultimately this leads to this area.  But ultimately, this leads, this area leads to systems of polynomial differential equations, which are quite difficult to solve. Thank you. Are there questions? Hi. I'm very interesting that, basically, you show that this robustness is very difficult.  you show that this robustness is there in this learning rules. What I wonder is, what is the gap and performance that you get if you go away from the classic.  you get if you go away from the classical backpropagation. Well, you see here in these simulations. Yes. Typically, the top curve is backpropagation.  the top curve is back propagation, so this may be amnist. So in twenty epoch, back propagation is essentially one hundred% and skipped or random  and skip or random back propagation maybe at ninety-five or ninety-six. So you see, it's a little bit slower, but ultimately it gets there. I mean, why ask?  But it gets there. I mean, why I ask is because basically reinforcement learning was the temporal difference thing. You also work with noisy gradients at one point. And empirically it shows that noisy gradients cost you something.  Is it great to cost you something in performance and training time? Do you think that this can be related here? Possibly. For simple systems like the one the long chain, we sort of know how long it will take.  odd number of such baits. Whereas here you just look at the pattern, and you see immediately whether the ones are clung together or not. That's your thinking when you use your visual system. But if now you put...  we can get some scaling there. But I don't expect that there is any fundamental difference. OK, thank you. So my first question, I guess, is answered already about  I guess this answer already about the speed of convergence. So what happens when you try it on an M-nist or one of these datasets? What you've seen the curves. Oh, yeah.  these datasets? What you see in the curves. So back propagation may converge in twenty epochs and one of the other variants may take fifty or one hundred epochs.  The other variants may take fifty or one hundred epochs, but you will get there in a reasonable time. Yeah, Max. Okay. So, as a follow-up, what about the work log time? I guess with randomized weights, you kind of save some computation, but you need long time  So is there some, do I get some speed improvements if I don't? If you use random weights, no, you'll be slower. You don't improve your speed. You mean your speed of conversion terms of the speed of conversion terms of the speed of conversion?  I mean your speed of conversion terms of epochs. But I need to do less computation, right? I guess, because I don't need to compute the gradients. Well, you see like,  to compute the gradients. Well, you still have to issue the random matrices. You still have to multiply by random matrices, multiply by derivative, multiply by random matrices. So you should do.  by random matrices. So if you do standard back propagation with random weights, the number of calculation is the same. It's just that you're using random weights instead of using.  using random weights, instead of using the transpose of the forward matrices. And Pierre, have you studied what kind of randomness  You know, what kind of randomness you should be using? It's very robust. What process have you changed process? Yeah, we changed process. Again, we tried so many times.  use your visual system. But if now you put yourself in the shoes of this neuron of this little logistic regression, the world looks completely different. There is no sense that be  process. Again we tried, so of course Gaussian, but then we tried sparse matrices with coin flips. That's fine. I'm not so much worried about convergent speeds, but I would be interested if  But I would be interested if you would use this tool not single networks, but ensemble of such networks with random back propagation type weights, whether these would have potential  whether these would have potentially a better performance and say an average of simply backpropagated neural networks. I see maybe a little bit relationships with the  I see maybe a little bit relationships, we know random weights that convey information back to maybe human creativity where maybe you need to make an error to find a good solution.  make an error to find a good solution. Yeah, so I haven't tried to do an ensemble of back propagations, but we have done dropout on the way back, which is a little bit more.  prop out on the way back, which is a little bit like averaging an ensemble. We don't see any, but it doesn't hurt you, but it doesn't buy you anything obvious, at least in the simulation.  So you can use, for example, message passing to train the layers of network or more general, you can use variation on inference, for example, your computer posterior  Whether it start to performSU Masterial it should supply for distribution even in the input to the rescue. We AC świativate produce more than Beck to target it. lito nماšne raz Artsog ehkvo dlga.好 ali.  structure to the one you proposed with the encoding net or the coding net or can skip connections between the two. So I was wondering is there any way of making a formal connection  There is no sense that the first bit is to the left of the second bit, to the left of the third bit, etc. That comes only from the visual system. In reality, this neuron has to learn the right way.  a formal connection between variational inference and what they are trying to do here? Possibly. I haven't really thought about it, but in a way, so in a very short time, I think it's  But in a way, so in a version of nothing color, for example, you will get a feedback which is exactly a quantity of information you can know.  of information, you can actually measure in NATSA and you use that quantity of information to correct and update your layers. So it feels very much like what you're trying to do here.  Thank you for one last question. Yeah, just a very simple question. So when does it fail? It cannot always work. Right. So I gave some example where it fails. For instance,  give some example where it fails. For instance, if you don't have, if on the learning channel, you completely forget the derivatives. You put random matrix.  derivatives, right? You put random matrices or even the transpose of the four-word matrices, but you don't multiply by derivatives, then it doesn't work. But an un-added example. In terms of applications, there's  And some example in terms of applications. So the data sets you have tried had always seemed to work. The variations that work, yes. They work on C-FAR. They throw it in.  that work, yes, they work on C-FAR, they try to we try on NIS, we try on another dataset that I didn't show, yes. Some are more finicky, you know, they require more.  more finicky, they require more hyperparameter tuning. Did you try just adapting the last layer? I mean, there's this echo state. If you adapt only the last layer on a difficult layer,  this neuron has to learn the right permutation of older and bits in order to solve this problem. So it turns out it's a very hard problem. And you get  it's very hard problem. And you get completely fooled by not thinking in the machine. Another example of thinking in the machine is drop out because you could  in the machine is dropout because you could have again looked at the world from the point of view of a neuron. Maybe you come up with the idea that neurons are quite  of course is the brain or a neuromorphic chip. What I really mean here is a physical neural network. It's not the fantasy that you use in your computer.  come up with the idea that neurons are quite faulty, maybe they don't work fifty% of the time, so let's do that during learning and that's exactly what dropout does. You remove fifty% or some other...  does. You remove fifty% or some other fraction of your neurons during learning. You're just the synapses and then another group of neurons is removed, etc. And you can  another group of neurons is removed, et cetera, and you keep repeating that. So drop out, you could view it as a thinking in the machine type of thinking. Now, not so much.  time of thinking. Now, note something interesting. When you do drop out at production time, your neurons are working perfectly. You just multiply the weights by the probabilities of dropping it.  by the probabilities of dropping and that's it. And so that maybe opens the door for having a better form of dropout, where you do dropout also at production.  where you do drop out also at production time. And I think maybe you may gain a very small amount in accuracy by doing that. Nobody has done that, but it's not.  that nobody has done that but it's probably a very small effect. It would require at production time to just average over a large number of networks rather than doing the pseudo-quiet.  rather than doing the pseudo quick average that you get by assuming that all the neurons are working properly. So that's dropout. Other example of something that I'm going to talk about is the same.  So, that's dropout. Other example of such thinking when you're looking at neurons would be for instance relaxing the weight sharing assumption.  the weight-sharing assumption. It's very unlikely that in biological networks, you have exact weight-sharing, for instance. So how can you have a convolution neural network that works? Well, if you...  the fantasy that you use in your computer when you're using a tensor flow and doing ResNet or something like that, that is not a real native neural network.  evolution your network that works well if you don't have exact weight sharing. Where you can show for instance that if you take a convolution your network and relax the weight sharing assumption, but you can't do that.  the weight-sharing assumption, but you initialize the weight sort of from the same, typically from this zero mean Gaussian with small standard deviation. You can get them to learn provided you  You can get them to learn provided you give them translated version of the examples so that every neuron, let's say in the first layer, sees roughly the same set of examples.  same set of examples. So these are just examples of thinking in the machine from the point of view of neurons. Now I want to go to the point of view from the point of view.  want to go to the point of view from the point of view of synapses, which I think is even more interesting. And this is, among other things, is going to help us answer questions like, what exactly  pass answer question like what exactly is Evian learning? Evian learning is an important concept, but it's somewhat murky, you know, you're on that wire together, far together. What does it mean?  together, what does that mean exactly? What is the relation between ambient learning and back propagation? His back propagation, heavy and for instance, a question that may seem  questions, a question that may seem somewhat strange but important. And why are there so few learning algorithms? Because really if you look at the link,  really if you look at the literature these are pretty much the only two algorithms that are available for training networks. So why are there so few algorithms? So if we put ourselves in the  a few algorithms. So if we put ourselves in the show, imagine that you are a synapse, the important thing I want to impress on you is that to understand  is not a real native neural network, right? You don't have synaptic weights, you don't have neurons, you have a fantasy of such objects implemented in a digital computer. And this makes a big difference.  In press on you is that to understand the world of signups which is such a small object, you need to rescale things so that they become more palatable to you. So I'm going to rescale things  of all to you. So I'm going to rescale things by factor of a million. The sign-ups is about ten to the minus seven. So you rescale it by a million. It's ten to the minus one meters.  million it's ten to the minus one meter so about ten centimeters the size of my fist and imagine Einstein is learning how to play the violin where is the bow of the violin well it's maybe a one meter  the violin, well it's maybe a one meter away, which when you rescale by ten to the six, that's a thousand kilometers away, so maybe in Paris, right? Or Rome, if you are travelling.  maybe in Paris, right, or Rome, if you're learning to write a bicycle. So you have these sign ups which is embedded in these deep circuits in  embedded in these deep circuits in the brain that has to decide whether to strengthen itself or not, or weaken itself. And then it has no notion of  itself. And then it has no notion of music, of violin, etc. Right? So how can it do that? And this is really the deep learning problem when you think about it.  deep learning problem when you think about it in biological terms. So that's just a rescaling. So this leads immediately to  the risk scaling. So this leads immediately to the notion of local learning, because sign apps in order to learn whatever the learning rule is, it has to depend on local variables that are available at the...  ako obalitja specs na načerik eh consequenome bil� je Nerdnedej po knjelaj mužč grindli cornersz Za adjavro,  digital computer. And this makes a big difference. And I think there is a lot to be learned by looking at deep learning in a nature.  that we're using, it means that you, one possible definition of local learning would be to say that you have a rule for adjusting a synaptic weights, which is some function of the  which is some function of local variables such as the precinaptic activity, the post-sinaptic activity, and maybe the way itself. That's a reasonable definition of what we're talking about.  That's a reasonable definition of what a local learning rule ought to be. You're welcome to have your own definition But within this formalism, I think this is very reasonable. Now if you are  This is very reasonable. Now, if you are, let's say you have a feed-forward network, you are in the output layer, then you may have targets. So maybe the targets could also be considered.  the target could also be considered as local variables in the output layer of a feed-for-word network. Okay? So that's the definition of a local learning  definition of a local learning rules. And I think this is nice because now you can separate the variables that are in the learning rule which need to be a local  in the learning rule, which need to be a local from the functional form of the learning rule. So I can decide what functional form the function f should have.  the function f should have, for instance we can work on polynomial learning rules, polynomials of low degree maybe up to six. That could be an interesting set of function, but in any case, we can use the function f to function f,  an interesting set of function. But in any case, we can stratify all possible learning rules, and we can study them one by one. So we did that for polynomial.com.  one by one. So we did that for polynomial learning rule of low degree for linear neurons in some cases you can do also non-linear neurons and you can study all these rules and their properties.  we learn by looking at deep learning in a native neural network in a physical system rather than in the digital simulation. By taking into account the  all these rules and their properties. And occasionally you can find rules that have some interesting capabilities, for instance, the Oja's rule, which is a quartic rule, actually, can extract  which is a quartic rule actually, can extract the principal component of the data. So these rules that you can study can extract simple statistics of the  study can extract simple statistics of the data such as the center of gravity or the principal component. So it's interesting to know that in which  It's interesting to know that in which rules converge, et cetera. But really what you care about is learning by combining these rules in a deep network. So here, we have a very important question.  finding these rules in a deep network. So here you have a deep fit for word network, and imagine that you're using local rules in the first layer, in the second layer, et cetera, all the way to the top layer.  layer, etc., all the way to the top layer. In the top layer, you can have targets, because the targets are available here, but not, of course, in the deeper layers of this feed-for-word network. Right? And the question is, what can you do with the  And the question is, what can you do with this system? What can you learn? This is actually an old idea that goes back at least to one thousand, nine hundred and eighty with Fukushima, who can  at least to one thousand, nine hundred and eighty with Fukushima who came up with this architecture which is nothing else than a convolutional neural network inspired by the work of ULV, etc. But in his favor he said,  will visual, et cetera. But in his favor, he said, well, we have adjustable ways between the layers, and we're going to learn using Hebs rule, some local learning rule.  using HabsRule, some local learning rule. Hab is a special case of local learning. Well, no one has ever been able to make this work, as far as I know. And the next question is,  And by taking into account the physical constraints of the real world on the real system, I think we can get new insights on the foundation of real life.  make this work as far as I know, and the reason is that it cannot work. So I'm going to tell you that if you stack things like this in a feed-for-one network, you have data, and you use  network, you have data, and you use Hebrew or any local rule here, here and here and here, etc., you'll never be able to learn interesting functions.  able to learn interesting functions. And the reason is actually quite simple. If you want to be able to learn things, a reasonable condition for that is to see.  reasonable condition for that is to say you should be able to reach ideally global minima or u-air or function but let's say at least critical points where well the gradient is zero so you're wrong  where the gradient is zero. So you write the equation of the gradient is equal to zero. Those of you who know these things well, of course, they understand that this is just writing back.  that this is just writing back propagation equals zero. So you can do it with large batches or just the full training set. So it's the  the full training set. So it's the average over the entire training set, let's say, of the, for a weight, w, ij connecting neuron j to neuron i in  ij connecting neuron i to neuron i in layer h, neuron j in layer h minus one. It's the sum of the precinaptic activity.  the sum of the precinaptic activity in layer h minus one multiplied by the post-cinaptic back-propagated error. This back-propagated error starts at the  This back-propagated error starts at the top of the network, depends on target-output, and then gets multiplied by all the weights in the reverse direction.  new insights on the foundation of learning and occasionally even find algorithms that can improve deep learning on computers. So how can we do this learning?  by all the weights in the reverse direction, in the propagation, all the weights of the network. In addition, every time you traverse a layer, you multiply by the derivative of the value.  the activation, not only the activation functions of that layer. So it's reasonable to assume that the solution to these equations, you have one such equation for the equation.  these equations, you have one such equation for each weight. So you may have a billion equations, right? The solutions have to depend, for instance, on the targets, because this term depends on the target.  because this term depends on the targets, might. If you do this local learning in fit-for-word mode, this Abyan learning apply layer by layer, you see immediately that the D-player will never depend on you.  immediately that the D-player will never depend on the targets. And so you cannot learn such function. By the way, in this term, there is also information about all  There is also information about all the weights above layer H, and we'll come back to that, because it would seem from this equation that you need to know.  from this equation that you need to know everything about all the weights above in order to find a solution and that's an important point that is actually not true. Okay, so have you any learning or  not true. Okay, so have you learning or more generally deep local learning, stocked local learning in a feed for one neural network cannot learn complex function?  neural network cannot learn complex functions. And so this leads to two new concepts, the concept of deep learning channel and local deep learning. So what  and local deep learning. So what do I mean by that? Well, we have seen that a deep weight has to depend on the targets, for instance, right? So there has to be...  on computers. So how can we do this learning in the machine? Well, the solution was found a long time ago by Albert Einstein. If you walk a few hundred meters,  is strangely in a way that you don't completely understand, this mechanism comes up into proof themselves to prove the theorems about the other two mechanisms which are near. So the other two mechanisms that  So the other two mechanisms that I'm going to talk about are multiplicative, just like our intuition sort of suggested.  And it's basically either the ability of some neurons to combine their outputs with the output of other neurons in a multiplicative way. So in the most simple case, a neuron produces an output and it ends up.  produces an output and it has the ability of multiplying the value of the target of the attending neuron output with its value. Or you have synaptic attention,  or you have synaptic attention where again the output of some neuron can be used to multiply now the synapses of some other neurons. And this is very much in the framework in the language of the brain.  in the in the you know, language of electrical engineering, it's really dating right it's dating the output or dating the synaptic weights about the neurons, I'm going to show them in the  neurons. I'm going to show them in sort of a large thing. So here is the output gating. What I mean by that is you have an attending neuron, here's neuron i, which is being attended by neuron j.  which is being attended by neuron J. The output of neuron J is the attention signal. And you see this output comes here. And it multiplies the output.  And it multiplies the output of neuron I. So this is outside of the standard model is this new algebraic operation where you allow neurons to multiply their outputs and then I'm going to get a little bit more.  multiply their outputs and then of course this multiplied output or I O J travels along the axon of neuron I towards say target neuron K here.  So roughly speaking, it's this idea that the brain somehow has this amazing ability to concentrate its computational power for lack of a better word.  to say a target on your own K here, which is going to receive as input this term here, which now of course is quite dry. So this is new in the standard model. You start saying.  the standard model, you start seeing these new quadratic terms appearing inside the neural net. Synaptic gating is a similar but now the attending neuron produces an output  out the atomic neuron produces an output OJ, which multiplies the synaptic weights. So this goes with the idea of the fast.  So this goes with the idea of the fast synaptic weights in Europe networks having weights with different timescales, but in this case you have weights that can be modified.  that can be modified on a rapid time scale compared to learning by the activity of the attending neurons. Right. Note that when you reach the neuron K, well gets there is the same thing.  Well, gets there is the same thing. It's the same thing in both cases. This new quadratic terms, but of course this mechanism is there is are different. And if you look at what happens.  And if you look at what happens on different axons emanating from neuron I or at least the different connection, I should say emanating from neuron I in this case you could assume that or your J.  In this case, you could assume that OIOJ is broadcasted everywhere to all the neurons, which are downstream of neuron I, whereas here it's a more precise mechanism that affects only this connection.  that affects only this connection between you and I and you on K, you don't have a broadcasting on all other connections. But both mechanisms are  But both mechanisms are quite interesting, can be added to the standard model, and this is exactly what happens in transformers. So that's what I'm going to do now. I'm going to move to transformers.  power for lack of a better word. We don't know exactly how it works, but to concentrate its resources on a particular stimulus in the sensory system, whether it's a visual system or  I'm going to move to Transformers, as I told you, this is the sort of cram, the like cram of how potential architecture is currently. And I'm going to unpack them.  in and show you how is the layout from a neural network point of view and show you where attention comes in and show you that it's exactly the type of mechanism that I just described.  of mechanism that I just described, which is the multiplication of outputs by outputs or multiplication of synaptic weights by outputs. Okay.  If you read the Littshound Transformers, they are made of basically building blocks and code blocks, the code blocks, but each one of these blocks has roughly the architecture.  roughly the architecture that is described here, which have been tried to layout as a Neurometry. And the idea is this, you have inputs in this block, which you should  inputs in this block, which you should think about of us vectors. For instance, in the first layer, these vectors would be  layer, these vectors would be representing the words in the incoming sequence using something like a word to vac some embedding of words into  to vac some embedding of words into vectors. All right. So these are the different vectors representing the different words in the sequence. And then there is a,  And then there is a circuit, let's suggest a one layer of weights that is transforming this vector here into three vectors, which are called QK and V, the query, the key and the value vector.  the key and the value vector, but these are just names. So the this vector here is transformed in one layer into three vectors q, k and v and then there's  visual stimulus or auditory stimulus for instance or also on some internal representation. On some thought, obviously if you're trying, let's say, to prove the mathematical TORM, you need a lot of concentration and  three vectors Q, K, and V. And then there is weight sharing. So the same circuit, the same weights are we use every position to produce these three vectors.  position to produce these three vectors. Right? So very simple initial stage. Then here you have the what they call the attention mechanisms, which is  they call the attention mechanisms, which is just taking all possible pairwise dot products between all Q vectors and all K vectors. Right. So if you have an  and all k vectors, right? So if you have n words, if the sequences length n, you're taking n squared dot products between all the q and all the k vectors.  between all the q and all the k vectors, all right. And then you apply a softmax to the rows of this matrix where you have put all these dot products.  where you have put all these dot products. And you use the soft marks to change the value of the weights in the top layer. So the top layer is just combine all the values.  So the top layer is just combining all the value vectors, right? Using some way, some synaptic weights, like you do in a neural network. But these weights, again, are modulated.  But these weights again are modulated by this softmax operation coming from here. So you see where the basic mechanisms coming. First of all, you have to come to  coming. First of all, you have to compute the dog products and when you want to compute, I hope it's in my next slide. Oops, let's see.  In order to compute the dot product of two vectors, let's say you vector x and a vector y, and let's say that x and y are the output of some neurons, you have to compute the sum of the x, y, x, and y.  you need a lot of concentration and you have to focus your attention on internal representation of mathematical concepts and so on and so forth. So that's the intuitive idea.  you have to compute the sum of the x, y, x, y, y, y. So you have to multiply x, y, y, y, y. You have to multiply the output of two neurons together.  output of the neurons together to compute one component of the dot product and do it again at times and then add them together, right? So computing a dot product  is an operation that naturally requires multiplying the outputs of two neurons or a bunch of neurons, one to one fashion, right. So that's what  you know, one to one fashion right. So that's where attention mechanisms are. But as I told you, you can do it without adding anything to the standard model, but as I will show you in a few slides, you get a circuit that is.  you get a circuit that is at least four or five layers deep in order to do that in the standard model. Here in one stage, in one layer, boom, you can compute the dot problem.  boom you can compute dot products between vectors. And then these dot products go through a softmax, this is sort of not new, but the softmax result, the result  But the soft marks result, the resulting weighting steam and remember the intuitive picture I had for the translation from English to French. This is great comes. You have a bunch.  this is great comes, you have a bunch of weights that are applied to these connections where maybe some V is enhanced and some Vs are suppressed by these weights here. And so you  these weights here. And so you have multiplication of synaptic weights by the output of these neurons, which is the synaptic gating mechanism right. So in some sense, to transform you use both  So in some sense, the transformers use both output gating to compute the dot products, the similarity. If you want it can be viewed as a similarity.  So that's the intuitive idea of attention and of course there's a question of how does it really work? And one way to study this problem of course is to go into neuroscience and neuroscientists that  If you want it can be viewed as a similarity if the vectors are normalized between the all the queue the query vectors and the key vectors and then it uses synaptic gating to gate these connections and combine the value.  these connections and combine the value vectors in a dynamic shifting way controlled by these attention mechanism, these attention weights. Okay.  mechanism, these attention worries. Okay? So, TrustFourner is built out of the fundamental building blocks, out of the quarks that I just described to you.  the quarks that I just described to you. One thing I want to impress on you that will be important for the applications is a little bit surprising is that the transformers are invariant.  transformers are invariant to permutations of their inputs. This seems a little bit strange because they come from an LP, in an LP the order of the words in  In NLP, the order of the words in a sentence is very important. But surprisingly, transformers don't care about the order. They throw it away. And you see it here.  I throw it away and you see it here because if I switch around the inputs, if I permute the inputs, you will get the same permutation in  You will get the same permutation in the first even layer because there is weight sharing. So the results will be the same but promoted in the same way. You will get the same permutation in the  you will get the same permutation in the dot products in the softmax and so the output will be identical. So you see that this whole block you  So you see that this whole block is invariant to permutation of the inputs, a little bit surprised for a surprising for language, but as we shall see very useful for applications where you need to  and neuroscientists that make stunning attention for a while. And the conclusion is that it's a very complex phenomenon. In fact, it's not a single thing.  applications where you need to be invariant to permutation of the inputs. So if your inputs are sets rather than sequences, that's exactly the kind of thing you want because you don't care about the order of the object.  care about the order of the objects in the input and so you want to sort of build that into your architecture. All right.  So here you see just the product mechanisms, you see a bunch of neurons, the attending neurons produce these output V one V two VN. These are the attended neurons with  These are the attended neurons without put x one x two xn. If I allow this new multiplication of v one with x one, v two with x two, vn with xn,  to Visex two, the end with Xn. And then I allow a summation here. This is how I get a dot product between X and V in a very compact form with this new mechanism.  Well, let me, you can of course also combine these with softmax, but let me just keep this. This is, as I told you, you can do the products within the standard model.  products within the standard model, but this is the circuit that you need in the SM in the standard model. If you wanted to do the dot product between you one you two you three let's say in the vector v of v one v two v three you have to be able to  you have to be able to do these kinds of things where you take the logarithm of the one and et cetera. And this firstly, you take the logarithm of everything.  Then here you add the logarithm. So you would add the logarithm of you one with the logarithm of the one to get these things. Then you would have a neuron with an exponential transfer function.  with an exponential transfer function that will give you back U one B one and then you would add everything at the top to get the dot product right. So you see a circuit will say one, two, three, four,  It's not a singular term, it is a variety of different processes and mechanisms as described in in this quote. So it is, there has certainly been progress, but it's a very complex  see of a circuit will say one, two, three layers or four layers, depending how you count it. And all this gets compacted into a much shallower building blocks.  into a much sharper building blocks, building block by having output gating mechanisms built in into your network.  So let me now show you a few applications of transformers to problems that I've nothing to do with language and I'm working now in the show.  with language and I'm working now in a few problems in physics as well as in chemistry. I don't have the time to go through these in detail but just give you the idea.  And you see for instance in chemistry you want to predict chemical reactions. So you have something like A plus B gives C plus B right the reactant should give some products A plus B gives C plus B.  Now, whether it's a plus b or b plus a, from chemistry point of view, it's the same thing. You're just at the same reactant slide. So the order is irrelevant when you're doing a chemical.  is irrelevant when you're doing a chemical reaction. And so that's why transformer are a useful technology, a useful type of circuit for chemical prediction. We're using them in physics in all kinds of ways.  We're using them in physics in all kinds of problems in physics. I'm going to give you just two examples very quickly. One is in the problems that come from  One is in the problems that come from collider physics, the large hydrogen collider for instance, where you are colliding protons at very high speeds.  colliding protons at very high speeds close to the speed of light. And you get a shower of particle that is produced around the  progress, but it's a very complex biological phenomena. So what we're going to do instead is to study attention within artificial neural networks, which is the learning.  that is produced around the collision point and these particles they decay, they are detected by by the detector and physics of course think about these things in terms of the  about these things in terms of the finite mind diagrams. So you see here partial finite diagram of the possible decay process. And without going in any of the details of the physics, this is a case where,  of the physics, this is a case where at the end of the decay you get in this fine-mine diagram.  in this in this fine mine diagram, you get six possibilities marked by BQQ and B prime QQ prime. This corresponds to major and  This corresponds to matter and antimatter because at this fundamental level there is symmetry between matter and antimatter. It's not true at the level of the entire universe as you  at the level of the entire universe as we know it, but at the elementary level there is this fundamental symmetry. So on one branch of the tree you're getting matter, particle of particles of matter,  particle of particles of matter on the other branch, you have to get the same particle, but the opposite charge corresponding to anti matter. And then you can see that whatever the meaning is for BQ and B.  The meaning is for BQ and B prime Q prime. On one branch of the tree, you have two cues, which are interchangeable, and you have on the other branch, you have two.  you have on the other branch, you have two symmetric Q primes that are also interchangeable. Now the data that you observe, what these BQs, etc are, in fact are vectors. So what you're observing are vectors  So what you're observing are vectors representing, for instance, the moment of the particles, vectors typically, let's say, of dimension four. So at the level of the observation, you may get a bunch of vectors.  which is the learning. And most of all, I'm going to focus on what should be the fundamental building blocks of attention. Regardless of whether it is a visual attention auditory attention,  the observation, you may get a bunch of vector of length four, ideally it should be six, but very often you have more than that because you have other things, you have garbage, you have noise, whatever.  So the input may look something like this where you have eight vectors of size four, these are called the jets for instance eight vectors of size four and your problem is to mash those eight.  And your problem is to mash those eight vectors to these three structures. So you have to identify among these eight vectors, which one are sort of garbage vectors that you can throw away.  that you can throw away, and then you have to identify which one are the B vectors and which one are the Q vectors and same thing for B prime and Q prime.  Now you see immediately that first of all the order of these vectors means nothing here. So the permutation invariance with respect to the order, that's already a good reason for using transformer. And furthermore in the final.  in transformer. And furthermore, in the finer structure, you see that you can permute the two cues. There is nothing canonical about these two cues. If I exchange these two vectors, it's the same.  by exchange these two vectors, it's the same structure. So you have permutation invariants at the level of the cues, and then you have also what is called the top symmetry, which is the top symmetry, which is the top symmetry.  So what is called the tau symmetry, which is the symmetry between matter and antimatter, if I call this thing Q prime and replace this one by Q.  It's still a valid tree, you know, there is not a canonical decision on which one is Q and which one is Q prime and the same for the for the for the beats. So plenty of permutations in these  So plenty of permutations in these problems that are essential and again something that can be handled very well with with transformers.  say visual attention auditory attention or some other kind of attention. I think all these ideas different types of attention at their core have a  And today I'm going to talk about attention. I'm using some vocabulary from physics, the quarks of attention, but it's really about the fundamental building blocks of attention in the same way that quarks.  with with transformers. There is different levels of symmetry so you really have to go through the details of our architecture to see how we handled up but you get the basic idea.  But you get the basic idea of permutation are very important. You can permute the inputs, the vectors in any possible way. And so, transformers are very, very suitable.  architecture for these kind of problems. These are just some of the results we're getting, where we can show that we're much better than the existing methods and especially much faster.  because the existing methods are looking at all possible assignments of jets to those free structures, so they are very slow.  three structures so they are very slow and transformers are actually much faster than some of the current approaches. So that's it.  some of the current approaches. So that's just to give you an idea without getting into the details. I'll show you another example of the completely at the other end of the spectrum, which is  with the neutron stars where you have, you know, mathematical models of the universe, which in this drawing have two parameters, let's say, so you have the equation of states with two parameters or four parameters, whatever.  two parameters or four parameters, whatever. And then from the theory of neutron star or nuclear theory, you can derive a curve, depending on where you are in this space.  depending on where you are in this space, you obtain a curve that tells you that in the universe, all the neutron stars have to be on this curve relating mass and radius, right?  relating mass and radius, right? So for each point in this space, you get a curve in the mass radius space that corresponds to the universe you live in.  certain basic mechanisms that we're going to try to identify. In particular, when we say  that corresponds to the universe you live in. Of course, and describing things in the forward direction, then you can sample neutron stars on this line. And you can add new instance parameters such as where is the distance of the neutron star?  know where is the distance of the neutron star from the point of observation, the temperature, things about dust, etc. And then from there you can produce a spectrum, you can do a simulation that  you can do a simulation that produced the corresponding spectrum in the x-ray channels, spectrum for the neutron stars. So of course, in reality,  So of course, in reality, we're going in the reverse. We are observing, we have data about some neutron star. No, is it about neutron star in the  know is it about Newton star in the universe and you want to go in the reverse direction to try to see where in this parameter space is our universe sitting right.  And so again, your input is the spectrum of let's say twenty Newton stars and there is nothing canonical about the order of this twenty spectra. It's just.  about the order of this twenty spectra. It's just the bag, a set of twenty spectra. And so again, we're using transformer to go, to do the inference from the,  inference from the twenty spectra or whatever number of spectra you have back to the parameter of states in these equations and you can see  in these equations. And you can see here the prediction of the first parameter, true value versus predicted value. And then the second parallel, which is quite good.  And then the second, which is quite good, the second parameter, the prediction is still pretty good, but there is more noise. And we're trying to reduce the noise in this prediction.  that it is at the exclusion of all other stimuli. This exclusion of all other stimuli means some kind of inhibition. It means that you're multiplying by zero.  Okay, and almost at the end now let me very quickly give you a sense of how you can try to build in mathematical theory of attention. And the tool I'm using to do this called capacity, it's a tool that we  is called capacity. It's a tool that we'll be using for quite some time. But the idea is very simple. If you have a class of functions, for instance, all the functions that are be computed by a neuron or by  be computed by a neuron or by an architecture, a neural architecture. You define the capacity as being the volume, the log base two of the volume of all the functions that can be computed.  all the functions that can be computed by your architecture. So if you're working with continuous neurons, you have of course,  with continuous neurons, you have of course, to define a notion of volume in a certain measure, a theoretical sense if you want. But I'm going to work with Boolean neurons, judgein with the, you know, I'm shaking.  with just going to use neuron that had a threshold function as their input so they're out to the zero one or minus one one right so linear threshold.  one or minus one one, right? So linear threshold functions. Then everything becomes Boolean and discrete, so you can actually count how many functions are in double.  how many functions are in double, and the log base two of the number of function that you get is what we call the capacity, the cardinal capacity.  the card you know, the capacity. Now measuring the volume of this ball is really gives you a sense of how powerful the ball is because the bigger the more powerful, but it's a very crude measure.  powerful, but it's a very crude measure, right? Because what you really want to know is what are the functions that are inside this mole, etc. and try to characterize them, which of course is  It means that you're multiplying by zero essentially all the other stimuli so that you can concentrate on what you're interested in. And maybe you're also announcing.  to characterize that, which of course is a very complex mathematical problem for say a deep architecture nobody's able to do that. This is very efficient. But still the reason why the log base two of the volume or the number of function  of the volume or the number of function is particularly important in terms of neural network. It's because from Shannon theory, you know,  It's because from Shannon theory, you know that this is the number of bits that you need to specify one of the functions within this ball, right. That's what  within this ball, right? That's what the log two of the cardinal obey is, the average number of bits needed to select a function within this ball.  the function within this role. And if you think about learning, learning is all about taking data information from the Trangesh set and using it to  from the Trangish set and using it to choose a function in the ball. The function that does the best approximation, right? You want to choose this red function that is  choose this red function that is the closestly of target function h, which may or may not be inside the ball, but that's not important, right? So  Learning in some way, it's all about extracting information from the data and pushing it into the weights of the neural network in order to select the red function of something similar to the  the red function or something similar to the red function. And so that's why the capacity is important for neural networks. And so the first thing you want to know is what is the capacity of a single neural network?  So, what is the capacity of a single euro on the linear threshold gate? So, there you are asking how many different linear threshold gates there are, how many Boolean functions.  And maybe you're also announcing what you're concentrating in. So it means that you must be able to multiply by zero certain things and enhance, which is multiplied by self factor of two or three.  that can be that are linear separable, right? A Boolean function is just the coloring of the cube of vectors of zero and one components into two colors, the plus one class and the minus one class.  and the minus one class. And if it's a linear threshold gate, it means that there is a hyperplane that separates the red dots from the blue dots. So you're asking, of all possible  So you're asking of all possible coloring of the cubes with two colors, red and blue, how many such colorings are linearly separable.  are linear and separable. That's the question. So it's a fundamental question about neural networks. It's not very well known, but I think it's one of the most important problems in neural network theory.  And in sixty-five, Cover had proved a number bound of n squared on the capacity of the single neuron, Moroga had a lower bound of one.five n squared. So there is a gap.  one half and square. So there is a gap. And the gap was actually solved in the late ladies by a Russian mathematician called Zuev was able to prove that the capacity is n squared. This means  that the total number of linear threshold function, boolean function is about two to the n squared, n squared as opposed to the total number of boolean functions, which is two to the  number of Boolean functions, which is two to the two to the n, right? Where if you have any inputs, any is the number of inputs, right? It makes a lot of sense because n squared means that you need n vectors of length  means that you need n vectors of length n to specify a linear threshold function. And if you write the linear system, you know, you get a sense that this is  you know, you get a sense that this is about the right what? You can do the same thing for polynomial threshold function.  factor of two or three, the thing that you're interested in. So already there you get an intuition that there is something, one one possible direction is to think that there is something  polynomial threshold function where you're taking a polynomial of degree D and then threshold threshold being it to plus one or minus one for instance.  to plus one or minus one. For instance, the upper bound there and the plus one over D factorial is the degree of the polynomial. This upper bound was actually my PhD thesis in the eighty s.  in the eighty s, there was a lower bound derived by SACs in the ninety s and recently Roman Varshthinin and I were able to prove that.  and I were able to prove the same thing, the generalization of the linear case, where you have this, basically, the capacity is n to the deepest one over the next one.  the capacity is then to the deepest one over difficulty. So the next question is how do you apply this to attention? So the most simple case and I'm just going to show you this most simple case is to imagine that you have a linear threshold.  We imagine that you have a linear threshold gate here. It's the blue neuron. So this is just a the sum of the W I XI and then you take the sign of that.  And then you take the sign of that. So you get an output that is plus one or minus one. You have the red neuron which is also linear threshold gate. It's the appending neuron. Right.  And then you take the product of the output of these two neurons to produce the final output. Right. Now it's a little bit different.  Now, it's a little bit different if you're using zero one neurons or one minus one neurons, because if you're taking zero one neurons,  Because if you're taking zero one new neurons, taking the product, right, as you see here, is the same thing as doing an end. If you have one one, you get a one, you know,  to think that there is something multiplicative about attention where you can multiply things by zero and inhibit them, suppress them, and then you can also multiply what you're interested in, enhance it by some.  If you have one one, you get a one in all the other cases, you get a zero. So it's like doing an ad. If instead you're using a minus one one formalism, if you have minus minus or plus plus,  If you have minus minus or plus plus, you get a plus. And if you have a mix, the minus plus or plus minus, you get a minus. So it's more like an x or or or the negation of an x. So there is a little different that.  So there is a little different there, but it creates two nice interesting problems. You know that for the first for the blue neuron you have to do the n square possibilities.  the capacities and square, the same thing for the red neuron, but then what happens when you multiply them together, right? So that's the new mathematical problem that you can try to solve to try  that you can try to solve to try to understand how many functions are created, how many Boolean functions are created in the circuits by allowing the multiplication of these two outputs.  the multiplication of these two outputs. And that's of course just the beginning you can do it with layers, you can imagine the output of this neuron multiplying the the snaps of an output.  on multiplying the synapse of another neuron to do synaptic gaining, but you can see how you can start creating all kinds of circuits where you want to compute their capacity.  And I'm just going to give you the punch line. In this case, this neuron is capacity n squared, this neuron is capacity n squared. And it turns out that the output it doesn't matter.  which one you're using as also capacity, as capacity equal to the sum. So the capacity, the answer to my question is two n squared up to a smaller, smaller terms. That's the capacity of this very simple circuit.  That's the capacity of this very simple circuit. You can do the same thing with polynomial gate. You can start the all kinds of other circuit, as I said. And that's what we're doing with this Roman and the ribos.  what you're interested in enhance it by some other factor. So that's the sort of thinking or idea we're going to develop. Now we're going to do this in artificial intelligence.  And that's what we're doing with this Roman and deriving all these results. And as I mentioned briefly already, the one, you know, surprising came to me is that the key mathematical tool for.  proving some of this result is actually an attention mechanism is the first type of attention mechanisms that I mentioned where which is inside the standard model where you have a bunch of neurons.  where you have a bunch of neurons that are inhibiting another neurons. I don't have time to show you that, but we're going to post the technical report on archive.  the technical report on archive in a few days, it's almost ready. And if you're interested, you can email me or you will see in archiving in a few days.  So very rapidly amount of time, so my conclusion, I have tried to show you that it's possible to produce a taxonomy of attention to really try to identify fundamental basic mechanisms that are used to be attention.  that I use to build attention. The interesting ones that I described to you are essentially output gating and synaptic gating, which are outside of standard model, so they extend the standard model.  So they extend the standard model towards using quadratic activations. You remember there were all those quadratic terms, both out to getting and synaptic getting on the fundamental building blocks.  fundamental building block of the transformer architectures, which have been very useful for NLP problems, but I've shown you that because they have this permutation,  because they have this permutation in variance property. They're also very good for other kinds of problems, for instance, in physics or chemistry. And then,  in physics or chemistry. And then I've tried to give you a sense of how you can begin building more precise mathematical theory of attention.  Now, we're going to do this in artificial neural networks, in particular in what I call, I'm going to call the standard model. And what is the standard model? Well, if you're telling my tutorial yesterday, the standard model is just.  The standard model is just the basic model for artificial neural networks where an artificial neural network is a network of very simple computing devices, which go back to macrulloc and pits.  which go back to macular can pits. Very simple model neurons if you want where a neuron is a computing device that operates by first they taking  of attention in the same way that quarks are the fundamental building blocks of other particles. So this is the outline of my talk. First I'm going to give a general introduction.  that operates by first taking the dot product of the incoming signal X with the weights W, the synaptic weights of the new ground. So you have a weighted average, which is called the activation.  which is called the activation of the neuron. And then this activation is sent through an activation or transfer function to produce an output.  transfer function to produce an output. And you have all kinds of activation functions such as seedmoyle activation function, tonnation and the logistic function.  or you can have a threshold function like the heavy side or the sine function or you can have a piecewise linear function like relu and so forth, right.  a function like Ray Liew and so forth, right? But this is what I call the standard model. Some of the mechanisms we're going to see are outside this definition of the standard model. Our new  definitional the standard model or new additions to the standard model. And it's important to keep in mind that the standard model, it's well known that the standard model as university.  that the standard model has universal approximation properties. In the sense that for instance, every boolean function is in the standard model, we're using linear threshold gates, and every continuous function over a  And every continuous function over a compact set, real value function over a compact set can be approximated within epsilon by a circuit by a network.  by a circuit, by a network that belongs to the standard model. It's very easy to show that. So whatever attention is or what we're going to do attention, it's not about  we're going to do a tangent. It's not about being able to compute functions, new functions that you cannot do with a standard model. Everything can be done within the standard model. It's all about  I'm going to give a general introduction to attention, the problem of attention, within the what I call the standard model borrowing again some language from physics.  done within the standard model. It's all about doing things in more efficient ways with less layers and things like that, right? But it's not about existential questions of the world.  the existential questions of what functions can be approximated with or without attach. Now it turns out of course that attention has actually been a very popular topic in  popular topic in deep learning over the past ten years, especially coming from the field of natural language processing of NLP, where people have been developing all of these  where people have been developing all kinds of so-called attention mechanisms in a sort of adop fashion, but in ways that have been very,  but in ways that have been very powerful for the applications. And the idea, the basic idea in natural language processing of  and applications of attentions in NLP is represented in this picture where you have at the bottom a sentence in English, I was your day, and then a translation in French,  And then a translation in French, which actually is not a very good one, but I downloaded this from the web, Commaus de Pastes-Journey, right. And the idea is that whatever the processing you're doing to translate  the process that you're doing to translate from English to French. When you're trying to produce a word, a position T in the output, you are  looking at the words in the input, but with some kind of waiting scheme, where certain words are more important than others. Okay, of course when you're trying to produce the word Jouenay, we're going to be in a little bit more.  you are trying to produce the word Jouenet, which means day in French. The word day in English is the most important one in the sentence and then the other ones.  very shortly what I mean by standard model. Then I will try to organize all possible attention mechanisms in a systematic way and provide some kind of taxonomy. I will then explain how attention works in transformers.  have less importance. In this case, you know, it's a same position as the last word in the sentence, but of course, as you change languages or sentence types, the, you know, tell us.  starts the you know, you don't have a one to one forest almonds between the location of words in the input sequence of course and the end and the same and the same position in the  and the same position in the output sequence, notwithstanding that the sequence to the different length as in this case, right? But you see this idea of attention  But you see this idea of attention, the ability to switch the importance that you give to the various streams of information that are coming through the deep learning system. So that's the idea that  So that's the idea that people have been implementing in natural language processing. There has been over the last decade a series of important publications on this topic. I've just.  publications on this topic I've just listed a few of them here. I have no time to go through them. But the important thing is that the sort of pinnacle of this  the sort of pinnacle of this movement of this trend in NLP is what are called transformer architectures, which are widely used today, which  which are widely used today, which have led to the state of the art results in LLP problems, in translation and other problems. This is what is used by older large  This is what is used by all the large companies, Google's and so forth. And these architectures are now implemented in the standard software frameworks.  in the standard software frameworks like that's a fluent pytorch so they are very widely used for natural language processing. But now they're also  how attention works in transformers, which are an important architecture in deep learning these days. I will demonstrate some applications of attention and in particular of transformers to some interesting problems.  language processing, but now they're also making their way into other areas. And I'll give you example of that. I'll give you examples of application of transformers to problems for instance in field.  Transformers to problems for instance in physics and explain why now they are they are making their way into other areas in in  in their interesting manner. If you read a paper about a transformer, you will see diagram of this kind, and you will vocabulary about query key and value vectors, if you're not in the NLP field,  vectors if you're not in the NLP field that will look a little bit strange. There will be matrix operation etc. So it's not clear if you read them if you glance through  If you read them, if you blast or such paper, it's not even clear that it's a neural network, right today. It's not they do not draw neural networks and what I will try to explain.  And what I will try to do is to unpack these architectures for those of you who have not really studied them in detail and show you how you can draw them, in fact, as a neural network. And dumb.  them in fact as a neural network and and then we will identify where the so-called attention mechanisms that the use are inside this  the use are inside these neural networks. All right. So let's try to organize now the fundamental building block of attention mechanisms.  whatever attention may be. And what I'm going to do to do in order to do that, I'm going to think in the following way, in the standard model that I described, there is three types of  that I described, there is three types of variables. You have the activation of the neurons, you have the output of the neurons, and you have the synaptic weights. So you have S, O, and W.  And then I will finish by showing you how you can begin to develop a mathematical theory of attention. So let me start from the beginning. What is attention?  So you have SO and W's. That's the only variables that are used in this in this in the standard mode.  You could even say that the activation and the output are redundant if the activation function is one to one, right? But let's imagine you have three variables, types, three types of variables.  three variables, types, three types of variables, activation outputs and synaptic weights. I'm going to try to classify all possible signals, whatever,  possible signals, whatever, attention signal, whatever they are, according to the origin of the signal. So when you have attention, you have  So when you have attention, you have an attention signal that is generated somewhere. I'm going to assume that it could be in the activations of neurons. It could be in the outputs of neurons.  It could be in the outputs of neurons, maybe it could be in the synaptic ways. So you have three possibilities. And then these attending signals have to reach their target and again the target could be of type S or  And again, the target could be of type S or W. You could even think about mixed schemes where the origin of the attending signal is both into in the output maybe an  is both into in the output maybe and the synaptic weights of some neurons. But that's a little bit strange and would of course create more cases.  and would of course create more cases. So I'm going to thank to these sort of homogeneous cases where the source of the signal is of one type.  source of the signal is of one type and the target of the signal is of another types. And then you have to specify how the signal, the attending signal, interacts with this target.  Of course, everyone of us knows, at least in an intuitive way, what attention is. It has been studied by psychologists for quite some time, so you see here the definition provided by James in the late one thousand, eight hundred s.  And already in the introduction I suggested that multiplication is likely to be a fundamental mechanism for attention. So we're going to look at multiplication. But in order to be complete, so...  But in order to be complete, the system not in some way, we're also going to look at additive interaction. Let's see what happens if we allow addition also to be one of the interaction.  also to be one of the interaction, the two interaction mechanisms. And so if you look at all these possible cases, you get eighteen cases, right, that you can study one by one.  that you can study one by one and it's an interesting thing. But already I'm going to shrink it down to six possibilities by making the assumption that the origin of the signal comes from the output of some neurons. So I'm assuming that  of some neurons. So I'm assuming that there are some neurons in the attention mechanism that produces an output and that's the attention signal.  output and that's the attention signal and this output then is going to interact with either S or W variables either multiplicatively or additively.  either multiplicatively or additively. Okay, so we have six possibilities. We have reduced things to six possibility and there are listed here.  And they're listed here where you have two mechanisms either through addition or multiplication and then you have three targets, the activations of some neurons or the  the activations of some neurons or the outputs of some neurons or the synapses of some neurons. Okay. Now I'm not going to go all three all six cases because not all of them.  all six cases because not all of them are interesting and some of them can be reduced to the others. But the three that are very important and that I'm going to use in the rest of the talk.  In fact, it's not obvious that there is something called attention, right, unless you start focusing on it. And also you see here two other things.  to use in the rest of the talk are what they call activation attention, where it's an additive mechanisms that are starting to get the activation of some neural. So this means that there are  some neurons. So this means that there are some neurons, the attending neurons, they produce some outputs, and these outputs are added to the activation of some  are added to the activation of some other neurons. Now, of course, this is nothing new. It's just part of the standard model. But the reason why this is important is because that is also  is because that is also one possible mechanism for inhibiting other neurons and doing the exclusion of other stimuli operation that is essential for attention. Now this happens when you have a lot of them.  for attention. Now this happens when you have, let's say, sigmoidal neurons. If the attending neurons send a very large negative signal to  large negative signal to the attendant neurons that have sigmoidal activation function. Of course, if you get a very large negative input, let's say you have the logistic, the output is going to be zero.  the logistic, the output is going to be zero, right? So one way of inhibiting neurons, those that should not be part of the attention mechanism is by sending them  the attention mechanism is by sending them a very large negative signal in the normal way within the standard model and shutting them down in that way. So that's why I'm including this.  that way. So that's why I'm including this additive mechanism in the list. It turns out that I'm not going to talk about it very much in the rest of the video.  talk about it very much in the rest of the talk because it's part of the center model, it's something new, but in the mathematical proofs of the theorems, very strangely in a way that  And also you see here two other definitions, for instance, the concentration of the awareness on some phenomenon to the exclusion of other stimuli.\n","{'%', '�', 'a', 'i', '?', 'd', 'م', 'š', 'Q', 'I', 'M', 'G', 'S', 'w', 'T', 'f', 'B', 'P', 'u', 'R', 'v', '2', 'j', 'D', 'ا', 'H', 'y', 'o', 'q', 'b', 'V', 'h', 'A', 'C', '.', 'x', 'l', 's', 'ś', 't', 'p', ',', 'č', '好', 'W', 'E', 'm', 'Y', 'r', ' ', 'X', 'L', \"'\", 'U', 'N', 'Z', 'z', 'g', '-', 'n', '1', 'e', 'c', 'k', 'K', 'ž', 'J', 'F', 'O'}\n"]}]},{"cell_type":"code","source":["dataset_vocab - tokenizer_vocab"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2mlfUUlPComd","executionInfo":{"status":"ok","timestamp":1684885394183,"user_tz":420,"elapsed":65,"user":{"displayName":"An Truong","userId":"05662548214724721674"}},"outputId":"480cf4ca-bb6b-4b21-8832-281c96f47f2a"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{' ', '%', '1', '2', 'č', 'ś', 'š', 'ž', 'ا', 'م', '好', '�'}"]},"metadata":{},"execution_count":14}]},{"cell_type":"code","source":["import os\n","import torch\n","from speechbrain.pretrained import EncoderClassifier\n","\n","spk_model_name = \"speechbrain/spkrec-xvect-voxceleb\"\n","\n","device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","speaker_model = EncoderClassifier.from_hparams(\n","    source=spk_model_name, \n","    run_opts={\"device\": device}, \n","    savedir=os.path.join(\"/tmp\", spk_model_name)\n",")\n","\n","def create_speaker_embedding(waveform):\n","    with torch.no_grad():\n","        speaker_embeddings = speaker_model.encode_batch(torch.tensor(waveform))\n","        speaker_embeddings = torch.nn.functional.normalize(speaker_embeddings, dim=2)\n","        speaker_embeddings = speaker_embeddings.squeeze().cpu().numpy()\n","    return speaker_embeddings"],"metadata":{"id":"50XDJHv0Cvit","colab":{"base_uri":"https://localhost:8080/","height":177,"referenced_widgets":["dc09363fc7054e51986cc8c0e9fbca06","510cf9f5620045a6be6de30bb53c85b5","2a065c12a0a74812b08070163b5d456f","c0b8287111f441b7b0bd6266a7877113","2cd0295726704782a836ef7c1b8e2e99","9d33dbc5d23a4bc0a3559e11a7bb7977","1a04da82e6fc464dadb4a0183097e2c8","bf0dcd8bbb794d159921050129f9242f","8dc49415296d410bb39a99941444955c","961fa51ebf464e00b47cb1d4adca5075","bd84ff61e0254fe5af6ed21f81a48b14","33a478781f1441f4a572175dfc99effd","e27335bf91a14fcf9c0666e1a099817d","a90a31e3257d4ee4a22645979b9af033","15e80cf16d844328a1eb468fe98841d1","71b9e52c263a4770afcf5d261710cff3","03531ee7155e43a4af67c38e3570d82d","1dea68e5a7a942e6955f8bfccc4018fb","ceca5a67ca384a0fb46bbc262d7b12b2","c11b42c13c144890a0a5ddebc1a81ccd","345410d0c83b4bc6bf78b48a4cb02bf4","0dc3fc0c2fed447dacdcee808c24a257","ad17c7593f7149d6960546a0af76697e","b29a7a23080b47a9a4b1e9eeb1c0d03b","febea99590de446f9e657dfa4e7a1e0c","2e2b091bbcb547b480464df3c7198ca5","1a6bdedc739e4def8f728b49dfd5bedb","9062d8f2a5eb4e6d8f6f923c600cb4fa","df740b0e83594211937ad0cf8daef22a","1c884795070d43e7b8e7a3e1aec3781d","8486ae6bee324cfcbb66ef122e03804e","1f0cea87d24541aaabaaafc595a04eec","13e292072f5e49a5b6fdd266bb4a42ad","0ba285903a274165bf45f9ba3e9248f4","8fc76868e81b44d3bde0a89daf70b3bf","4b583464f1184a14bdfde52a6d2939a8","846fbd870a7a4b7eaad632333c143643","abda540af1d54f7084fbf4a1e2c102af","66eea50b658848a7a2ef6ae3f878b1b3","a4848b2e22704e0db39286316a968e47","3a926a31134b4f22ae9d9c8bfffa9fd2","cf7a9f2680db4e73b568b734bc06c96e","59ca467caa3a4f82840a33e5fbc51ce4","0b7fea5795f642a89c5f7710532f4372","1f81fdf921ee475391808ba5fe3e8ae2","9ace3a7b08d34c72b58b78df1252d45d","c24576641bba45789cd052c5573db176","d14ba065e61b423d95812cdc5866e605","806c1071c5e44f8c99439ad23406c0ca","fccd3bc99a5c4a0a8609943c4dceceab","45d5d3d67d8f4721b05ce3bb5e6d2bd6","5334693c22aa42fbb30f096be91dbce9","94b9110df3674736a61f67619c803be9","f8ff5b4e316148f3aeaf3322ed44c0e8","a61b373ce80843c08ea39ca4001b38aa"]},"executionInfo":{"status":"ok","timestamp":1684885404293,"user_tz":420,"elapsed":10172,"user":{"displayName":"An Truong","userId":"05662548214724721674"}},"outputId":"db71bd42-a14b-418c-ebd6-07650a726900"},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":["Downloading (…)ain/hyperparams.yaml:   0%|          | 0.00/2.04k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"dc09363fc7054e51986cc8c0e9fbca06"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Downloading embedding_model.ckpt:   0%|          | 0.00/16.9M [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"33a478781f1441f4a572175dfc99effd"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Downloading mean_var_norm_emb.ckpt:   0%|          | 0.00/3.20k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ad17c7593f7149d6960546a0af76697e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Downloading classifier.ckpt:   0%|          | 0.00/15.9M [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0ba285903a274165bf45f9ba3e9248f4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Downloading (…)in/label_encoder.txt:   0%|          | 0.00/129k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1f81fdf921ee475391808ba5fe3e8ae2"}},"metadata":{}}]},{"cell_type":"code","source":["dataset"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8STOrM2rDba4","executionInfo":{"status":"ok","timestamp":1684885404294,"user_tz":420,"elapsed":24,"user":{"displayName":"An Truong","userId":"05662548214724721674"}},"outputId":"75977aa0-c4ce-45e0-aa62-5752d19d6628"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["DatasetDict({\n","    train: Dataset({\n","        features: ['audio', 'transcription'],\n","        num_rows: 557\n","    })\n","})"]},"metadata":{},"execution_count":16}]},{"cell_type":"code","source":["torch.cuda.empty_cache()"],"metadata":{"id":"NjrKkmGpDmX_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def prepare_dataset(example):\n","    # Load the audio data; if necessary, resample the audio to 16kHz\n","    audio = example[\"audio\"]\n","\n","    # Feature extraction and tokenization\n","    example = processor(\n","        text=example[\"transcription\"],\n","        audio_target=audio[\"array\"],\n","        sampling_rate=audio[\"sampling_rate\"],\n","        return_attention_mask=False,\n","    )\n","\n","    # Strip off the batch dimension\n","    example[\"labels\"] = example[\"labels\"][0]\n","    \n","    return example"],"metadata":{"id":"neFNnaU5EE9E"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#dataset['train']['audio']"],"metadata":{"id":"jjOZPWUBHpiQ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["dataset"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"R_jijm-2Iud1","executionInfo":{"status":"ok","timestamp":1684885404296,"user_tz":420,"elapsed":22,"user":{"displayName":"An Truong","userId":"05662548214724721674"}},"outputId":"ee45f8d1-2b54-47f0-d7b3-16c4e2197dd6"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["DatasetDict({\n","    train: Dataset({\n","        features: ['audio', 'transcription'],\n","        num_rows: 557\n","    })\n","})"]},"metadata":{},"execution_count":20}]},{"cell_type":"code","source":["audio_wav = [audio['array'] for audio in dataset['train']['audio']]"],"metadata":{"id":"6xJ4UZsSEk-Z"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import scipy.signal\n","import numpy as np\n","\n","def resample_waveform(waveform, target_length):\n","    resampled_wav = []\n","    for wav in waveform:\n","      current_length = len(wav)\n","      resampled = scipy.signal.resample(wav, target_length)\n","      resampled_wav.append(resampled)\n","\n","    return np.array(resampled_wav)\n","\n","# Example usage\n","waveform = audio_wav\n","target_length = 192960 \n","resampled_waveform = resample_waveform(waveform, target_length)"],"metadata":{"id":"QbmI2KNXTxNx"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["speaker_embeddings = create_speaker_embedding(resampled_waveform)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6gSNbobaESi-","executionInfo":{"status":"ok","timestamp":1684885471390,"user_tz":420,"elapsed":8513,"user":{"displayName":"An Truong","userId":"05662548214724721674"}},"outputId":"39fdec36-65b1-41f8-c7d0-0661e7fe7310"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/torch/functional.py:641: UserWarning: stft with return_complex=False is deprecated. In a future pytorch release, stft will return complex tensors for all inputs, and return_complex=False will raise an error.\n","Note: you can still call torch.view_as_real on the complex output to recover the old return format. (Triggered internally at ../aten/src/ATen/native/SpectralOps.cpp:862.)\n","  return _VF.stft(input, n_fft, hop_length, win_length, window,  # type: ignore[attr-defined]\n"]}]},{"cell_type":"code","source":["import datasets\n","\n","# Create a new dataset with speaker embeddings\n","new_dataset = datasets.Dataset.from_dict({\n","    'audio': dataset['train']['audio'],\n","    'transcription': dataset['train']['transcription'],\n","    'speaker_embeddings': speaker_embeddings\n","})\n","\n","# Replace the original dataset with the new dataset\n","dataset['train'] = new_dataset"],"metadata":{"id":"BStcCwfYU-xy"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["dataset"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"S3TUNl76WnlS","executionInfo":{"status":"ok","timestamp":1684885495871,"user_tz":420,"elapsed":23,"user":{"displayName":"An Truong","userId":"05662548214724721674"}},"outputId":"c3113fed-cb70-4587-d764-387e66af90cf"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["DatasetDict({\n","    train: Dataset({\n","        features: ['audio', 'transcription', 'speaker_embeddings'],\n","        num_rows: 557\n","    })\n","})"]},"metadata":{},"execution_count":25}]},{"cell_type":"code","source":["torch.cuda.empty_cache()"],"metadata":{"id":"sye_gvFuVIrI"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["dataset = dataset.map(prepare_dataset)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":49,"referenced_widgets":["badfe1e7892e4f6d98de990512cdd66a","3fd1a7bf2e384a9389a73ac7036fcdfc","640664c83922472996b73d7c5649b56d","0b43d3c702274b998de9a9cea487764e","2d06872741424a568f78ffecdbfcb09f","a2b6607e896c44dd85392da6603f17d1","dd9fbb00290a4b5a9ab4665c93062da1","ae9eb8ca1c914d9c90e3408a6a342c8d","dc26466af71742908f7c78f7d452ddcb","4832fceac7f545a9a68a5f57ed31edc6","95782bc14f8041208e139eb1eb21d4fe"]},"id":"XhBPwRWcDoga","outputId":"6126a41f-066a-41f9-eb86-c568776a52c4"},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":["Map:   0%|          | 0/557 [00:00<?, ? examples/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"badfe1e7892e4f6d98de990512cdd66a"}},"metadata":{}}]},{"cell_type":"code","source":["def is_not_too_long(input_ids):\n","     input_length = len(input_ids)\n","     return input_length < 200\n","\n","dataset_test = dataset.filter(is_not_too_long, input_columns=[\"input_ids\"])\n","dataset_test"],"metadata":{"id":"bfBHOsRnVWNQ"},"execution_count":null,"outputs":[]}]}